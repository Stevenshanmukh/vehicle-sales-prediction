{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "233362ae",
   "metadata": {},
   "source": [
    "# Notebook 04: Dashboard Preparation + Artifact Packaging\n",
    "\n",
    "**Project:** Vehicle Sales & Market Insights  \n",
    "**Purpose:** Prepare optimized artifacts for Streamlit dashboard deployment\n",
    "\n",
    "## Objective\n",
    "Create a complete, lightweight dashboard package:\n",
    "- Simplified model bundle for fast inference\n",
    "- Lookup tables for dropdowns (makes, models, states)\n",
    "- Feature importance summaries\n",
    "- Sample predictions for demonstration\n",
    "- Performance metrics dashboard data\n",
    "- Pre-computed visualizations\n",
    "- Complete deployment bundle\n",
    "\n",
    "## Dashboard Features to Support\n",
    "1. **Price Prediction Tool** - Interactive form with real-time predictions\n",
    "2. **Model Insights** - Feature importance and performance metrics\n",
    "3. **Market Analysis** - Price trends by segment\n",
    "4. **Performance Dashboard** - Accuracy by state/make/body type\n",
    "5. **Sample Predictions** - Example vehicles with explanations\n",
    "\n",
    "## Step 1: Environment Setup\n",
    "Load all necessary artifacts and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92d8ade4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DASHBOARD ARTIFACT PREPARATION\n",
      "================================================================================\n",
      "\n",
      "✓ Directories created\n",
      "\n",
      "Loading artifacts...\n",
      "  ✓ Model loaded\n",
      "  ✓ Metadata loaded\n",
      "  ✓ Label encoders loaded\n",
      "  ✓ Dataset loaded\n",
      "  ✓ Explainability data loaded\n",
      "\n",
      "================================================================================\n",
      "All artifacts loaded successfully!\n",
      "Model: XGBoost Regressor\n",
      "Test Performance: $887.48 MAE, 0.9682 R²\n"
     ]
    }
   ],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Model\n",
    "import xgboost as xgb\n",
    "\n",
    "# Settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DASHBOARD ARTIFACT PREPARATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create dashboard artifacts directory\n",
    "os.makedirs('app/artifacts', exist_ok=True)\n",
    "os.makedirs('app/assets', exist_ok=True)\n",
    "\n",
    "print(\"\\n✓ Directories created\")\n",
    "\n",
    "# Load all necessary files\n",
    "print(\"\\nLoading artifacts...\")\n",
    "\n",
    "# Model\n",
    "with open('models/final/xgboost_optimized.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "print(\"  ✓ Model loaded\")\n",
    "\n",
    "# Metadata\n",
    "with open('models/final/model_metadata.pkl', 'rb') as f:\n",
    "    metadata = pickle.load(f)\n",
    "print(\"  ✓ Metadata loaded\")\n",
    "\n",
    "# Label encoders\n",
    "with open('models/preprocessing/label_encoders.pkl', 'rb') as f:\n",
    "    label_encoders = pickle.load(f)\n",
    "print(\"  ✓ Label encoders loaded\")\n",
    "\n",
    "# Cleaned data\n",
    "df = pd.read_csv('data/processed/car_prices_cleaned.csv')\n",
    "print(\"  ✓ Dataset loaded\")\n",
    "\n",
    "# Explainability summary\n",
    "with open('artifacts/explainability/explainability_summary.pkl', 'rb') as f:\n",
    "    explainability = pickle.load(f)\n",
    "print(\"  ✓ Explainability data loaded\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"All artifacts loaded successfully!\")\n",
    "print(f\"Model: {metadata['model_type']}\")\n",
    "print(f\"Test Performance: ${metadata['performance']['test_mae']:,.2f} MAE, {metadata['performance']['test_r2']:.4f} R²\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb773003",
   "metadata": {},
   "source": [
    "## Step 2: Create Lookup Tables for Dashboard\n",
    "\n",
    "Generate lookup tables for dropdown menus:\n",
    "- Unique makes, models, body types, states\n",
    "- Value ranges for numeric inputs\n",
    "- Encoded mappings for predictions\n",
    "- Popular combinations for quick selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d50eb72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATING LOOKUP TABLES\n",
      "================================================================================\n",
      "Extracting unique values for categorical features:\n",
      "\n",
      "  make: 67 unique values\n",
      "  body: 46 unique values\n",
      "  transmission: 2 unique values\n",
      "  state: 64 unique values\n",
      "  color: 46 unique values\n",
      "  interior: 17 unique values\n",
      "  seller_grouped: 101 unique values\n",
      "  model_grouped: 201 unique values\n",
      "  trim_grouped: 101 unique values\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Creating numeric feature ranges:\n",
      "\n",
      "  year:\n",
      "    Range: 1,982 to 2,015\n",
      "    Default: 2,012\n",
      "  condition:\n",
      "    Range: 1.0 to 49.0\n",
      "    Default: 35.0\n",
      "  odometer:\n",
      "    Range: 1 to 500,000\n",
      "    Default: 52,255\n",
      "  mmr:\n",
      "    Range: 25.0 to 182,000.0\n",
      "    Default: 12,250.0\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Creating popular vehicle presets:\n",
      "\n",
      "  Top 20 popular combinations created\n",
      "  Example: Nissan Altima Sedan\n",
      "           (18176 vehicles in dataset)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Creating state-specific data:\n",
      "\n",
      "  State statistics created for 64 states\n",
      "  Top state: Fl (82945 vehicles)\n",
      "\n",
      "================================================================================\n",
      "✓ Lookup tables saved:\n",
      "  - app/artifacts/lookup_tables.pkl (complete)\n",
      "  - app/artifacts/lookup_tables.json (simplified)\n"
     ]
    }
   ],
   "source": [
    "print(\"CREATING LOOKUP TABLES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Get unique values for each categorical feature\n",
    "lookup_tables = {}\n",
    "\n",
    "categorical_features = ['make', 'body', 'transmission', 'state', 'color', \n",
    "                       'interior', 'seller_grouped', 'model_grouped', 'trim_grouped']\n",
    "\n",
    "print(\"Extracting unique values for categorical features:\\n\")\n",
    "\n",
    "for feature in categorical_features:\n",
    "    unique_values = sorted(df[feature].unique())\n",
    "    lookup_tables[feature] = unique_values\n",
    "    print(f\"  {feature}: {len(unique_values)} unique values\")\n",
    "\n",
    "# 2. Create value ranges for numeric inputs\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Creating numeric feature ranges:\\n\")\n",
    "\n",
    "numeric_ranges = {\n",
    "    'year': {\n",
    "        'min': int(df['year'].min()),\n",
    "        'max': int(df['year'].max()),\n",
    "        'default': int(df['year'].median())\n",
    "    },\n",
    "    'condition': {\n",
    "        'min': float(df['condition'].min()),\n",
    "        'max': float(df['condition'].max()),\n",
    "        'default': float(df['condition'].median())\n",
    "    },\n",
    "    'odometer': {\n",
    "        'min': int(df['odometer'].min()),\n",
    "        'max': int(df['odometer'].max()),\n",
    "        'default': int(df['odometer'].median()),\n",
    "        'step': 1000\n",
    "    },\n",
    "    'mmr': {\n",
    "        'min': float(df['mmr'].min()),\n",
    "        'max': float(df['mmr'].max()),\n",
    "        'default': float(df['mmr'].median())\n",
    "    }\n",
    "}\n",
    "\n",
    "for feature, ranges in numeric_ranges.items():\n",
    "    print(f\"  {feature}:\")\n",
    "    print(f\"    Range: {ranges['min']:,} to {ranges['max']:,}\")\n",
    "    print(f\"    Default: {ranges['default']:,}\")\n",
    "\n",
    "# 3. Create popular vehicle combinations (top 20)\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Creating popular vehicle presets:\\n\")\n",
    "\n",
    "popular_combos = df.groupby(['make', 'model_grouped', 'body']).agg({\n",
    "    'sellingprice': ['count', 'mean'],\n",
    "    'odometer': 'mean',\n",
    "    'year': lambda x: x.mode()[0] if len(x.mode()) > 0 else x.median(),\n",
    "    'condition': 'median',\n",
    "    'mmr': 'mean'\n",
    "}).round(0)\n",
    "\n",
    "popular_combos.columns = ['count', 'avg_price', 'avg_odometer', 'typical_year', 'typical_condition', 'avg_mmr']\n",
    "popular_combos = popular_combos.sort_values('count', ascending=False).head(20).reset_index()\n",
    "\n",
    "print(f\"  Top 20 popular combinations created\")\n",
    "print(f\"  Example: {popular_combos.iloc[0]['make']} {popular_combos.iloc[0]['model_grouped']} {popular_combos.iloc[0]['body']}\")\n",
    "print(f\"           ({int(popular_combos.iloc[0]['count'])} vehicles in dataset)\")\n",
    "\n",
    "# 4. Create state-specific defaults\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Creating state-specific data:\\n\")\n",
    "\n",
    "state_data = df.groupby('state').agg({\n",
    "    'sellingprice': ['count', 'mean', 'median'],\n",
    "    'odometer': 'mean',\n",
    "    'vehicle_age': 'mean'\n",
    "}).round(0)\n",
    "state_data.columns = ['count', 'avg_price', 'median_price', 'avg_odometer', 'avg_age']\n",
    "state_data = state_data.sort_values('count', ascending=False)\n",
    "\n",
    "print(f\"  State statistics created for {len(state_data)} states\")\n",
    "print(f\"  Top state: {state_data.index[0]} ({int(state_data.iloc[0]['count'])} vehicles)\")\n",
    "\n",
    "# 5. Save all lookup tables\n",
    "lookup_package = {\n",
    "    'categorical_options': lookup_tables,\n",
    "    'numeric_ranges': numeric_ranges,\n",
    "    'popular_vehicles': popular_combos.to_dict('records'),\n",
    "    'state_data': state_data.to_dict('index'),\n",
    "    'label_encoders': {k: {str(v): i for i, v in enumerate(le.classes_)} \n",
    "                       for k, le in label_encoders.items()}\n",
    "}\n",
    "\n",
    "with open('app/artifacts/lookup_tables.pkl', 'wb') as f:\n",
    "    pickle.dump(lookup_package, f)\n",
    "\n",
    "# Also save as JSON for easy access\n",
    "lookup_json = {\n",
    "    'categorical_options': {k: list(v) for k, v in lookup_tables.items()},\n",
    "    'numeric_ranges': numeric_ranges,\n",
    "    'popular_vehicles': popular_combos[['make', 'model_grouped', 'body', 'typical_year', \n",
    "                                        'avg_odometer', 'avg_price']].head(10).to_dict('records')\n",
    "}\n",
    "\n",
    "with open('app/artifacts/lookup_tables.json', 'w') as f:\n",
    "    json.dump(lookup_json, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ Lookup tables saved:\")\n",
    "print(\"  - app/artifacts/lookup_tables.pkl (complete)\")\n",
    "print(\"  - app/artifacts/lookup_tables.json (simplified)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901bd7de",
   "metadata": {},
   "source": [
    "## Step 3: Create Sample Predictions with Explanations\n",
    "\n",
    "Generate diverse sample predictions for dashboard demonstration:\n",
    "- Budget vehicles (<$10k)\n",
    "- Mid-range vehicles ($10k-$25k)\n",
    "- Luxury vehicles (>$25k)\n",
    "- Different body types and conditions\n",
    "- Include actual vs predicted prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7345f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATING SAMPLE PREDICTIONS\n",
      "================================================================================\n",
      "Selecting diverse sample vehicles...\n",
      "\n",
      "Sample Predictions:\n",
      "\n",
      "Budget Vehicle:\n",
      "  2007 Subaru Forester (Sedan)\n",
      "  Odometer: 166,658 miles | Condition: 19.0\n",
      "  Actual: $4,000 | Predicted: $3,885\n",
      "  Error: $115 (2.9%)\n",
      "\n",
      "Mid-Range Sedan:\n",
      "  2013 Toyota Camry (Sedan)\n",
      "  Odometer: 41,845 miles | Condition: 49.0\n",
      "  Actual: $13,900 | Predicted: $14,485\n",
      "  Error: $-585 (-4.2%)\n",
      "\n",
      "Luxury Vehicle:\n",
      "  2015 Nissan Other_Model (Coupe)\n",
      "  Odometer: 73 miles | Condition: 44.0\n",
      "  Actual: $84,500 | Predicted: $84,391\n",
      "  Error: $109 (0.1%)\n",
      "\n",
      "Popular SUV:\n",
      "  2012 Gmc Acadia (Suv)\n",
      "  Odometer: 36,711 miles | Condition: 39.0\n",
      "  Actual: $22,000 | Predicted: $22,311\n",
      "  Error: $-311 (-1.4%)\n",
      "\n",
      "Low Mileage Recent:\n",
      "  2014 Ford Fusion (Sedan)\n",
      "  Odometer: 14,358 miles | Condition: 35.0\n",
      "  Actual: $15,600 | Predicted: $16,145\n",
      "  Error: $-545 (-3.5%)\n",
      "\n",
      "High Mileage Older:\n",
      "  1999 Honda Accord (Sedan)\n",
      "  Odometer: 285,752 miles | Condition: 19.0\n",
      "  Actual: $1,100 | Predicted: $704\n",
      "  Error: $396 (36.0%)\n",
      "\n",
      "================================================================================\n",
      "✓ 6 sample predictions created and saved\n",
      "  - app/artifacts/sample_predictions.pkl\n",
      "  - app/artifacts/sample_predictions.json\n"
     ]
    }
   ],
   "source": [
    "print(\"CREATING SAMPLE PREDICTIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Select diverse sample vehicles\n",
    "print(\"Selecting diverse sample vehicles...\\n\")\n",
    "\n",
    "# Encode the dataset\n",
    "df_encoded = df.copy()\n",
    "for col in label_encoders.keys():\n",
    "    df_encoded[col] = label_encoders[col].transform(df_encoded[col].astype(str))\n",
    "\n",
    "# Get features\n",
    "features = metadata['features']\n",
    "X = df_encoded[features]\n",
    "y = df['sellingprice']\n",
    "\n",
    "# Generate predictions\n",
    "y_pred = model.predict(X)\n",
    "df['predicted_price'] = y_pred\n",
    "df['prediction_error'] = df['sellingprice'] - y_pred\n",
    "df['error_pct'] = (df['prediction_error'] / df['sellingprice'] * 100)\n",
    "\n",
    "# Sample selection criteria\n",
    "samples = []\n",
    "\n",
    "# 1. Budget car (good prediction)\n",
    "budget = df[(df['sellingprice'] < 10000) & \n",
    "            (abs(df['error_pct']) < 5)].sample(1, random_state=42)\n",
    "samples.append(('Budget Vehicle', budget))\n",
    "\n",
    "# 2. Mid-range sedan (typical)\n",
    "midrange = df[(df['sellingprice'] >= 10000) & \n",
    "              (df['sellingprice'] <= 25000) & \n",
    "              (df['body'] == 'Sedan')].sample(1, random_state=43)\n",
    "samples.append(('Mid-Range Sedan', midrange))\n",
    "\n",
    "# 3. Luxury vehicle\n",
    "luxury = df[df['sellingprice'] > 50000].sample(1, random_state=44)\n",
    "samples.append(('Luxury Vehicle', luxury))\n",
    "\n",
    "# 4. SUV\n",
    "suv = df[(df['body'] == 'Suv') & \n",
    "         (df['sellingprice'] >= 15000) & \n",
    "         (df['sellingprice'] <= 30000)].sample(1, random_state=45)\n",
    "samples.append(('Popular SUV', suv))\n",
    "\n",
    "# 5. Low mileage newer car\n",
    "low_mile = df[(df['odometer'] < 30000) & \n",
    "              (df['vehicle_age'] < 3)].sample(1, random_state=46)\n",
    "samples.append(('Low Mileage Recent', low_mile))\n",
    "\n",
    "# 6. High mileage older car\n",
    "high_mile = df[(df['odometer'] > 150000) & \n",
    "               (df['vehicle_age'] > 8)].sample(1, random_state=47)\n",
    "samples.append(('High Mileage Older', high_mile))\n",
    "\n",
    "# Create sample predictions dataframe\n",
    "sample_predictions = []\n",
    "\n",
    "print(\"Sample Predictions:\\n\")\n",
    "for category, sample_df in samples:\n",
    "    row = sample_df.iloc[0]\n",
    "    \n",
    "    sample_pred = {\n",
    "        'category': category,\n",
    "        'make': row['make'],\n",
    "        'model': row['model_grouped'],\n",
    "        'year': int(row['year']),\n",
    "        'body': row['body'],\n",
    "        'transmission': row['transmission'],\n",
    "        'odometer': int(row['odometer']),\n",
    "        'condition': float(row['condition']),\n",
    "        'state': row['state'],\n",
    "        'color': row['color'],\n",
    "        'interior': row['interior'],\n",
    "        'mmr': float(row['mmr']),\n",
    "        'actual_price': float(row['sellingprice']),\n",
    "        'predicted_price': float(row['predicted_price']),\n",
    "        'error': float(row['prediction_error']),\n",
    "        'error_pct': float(row['error_pct']),\n",
    "        'vehicle_age': int(row['vehicle_age'])\n",
    "    }\n",
    "    \n",
    "    sample_predictions.append(sample_pred)\n",
    "    \n",
    "    print(f\"{category}:\")\n",
    "    print(f\"  {row['year']} {row['make']} {row['model_grouped']} ({row['body']})\")\n",
    "    print(f\"  Odometer: {int(row['odometer']):,} miles | Condition: {row['condition']}\")\n",
    "    print(f\"  Actual: ${row['sellingprice']:,.0f} | Predicted: ${row['predicted_price']:,.0f}\")\n",
    "    print(f\"  Error: ${row['prediction_error']:,.0f} ({row['error_pct']:.1f}%)\")\n",
    "    print()\n",
    "\n",
    "# Save samples\n",
    "with open('app/artifacts/sample_predictions.pkl', 'wb') as f:\n",
    "    pickle.dump(sample_predictions, f)\n",
    "\n",
    "with open('app/artifacts/sample_predictions.json', 'w') as f:\n",
    "    json.dump(sample_predictions, f, indent=2)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"✓ {len(sample_predictions)} sample predictions created and saved\")\n",
    "print(\"  - app/artifacts/sample_predictions.pkl\")\n",
    "print(\"  - app/artifacts/sample_predictions.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04882110",
   "metadata": {},
   "source": [
    "## Step 4: Create Performance Dashboard Data\n",
    "\n",
    "Pre-compute performance metrics and aggregations for dashboard visualizations:\n",
    "- Overall model metrics\n",
    "- Performance by segment (state, make, body type)\n",
    "- Price distribution analysis\n",
    "- Error analysis summaries\n",
    "- Feature importance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e236ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATING PERFORMANCE DASHBOARD DATA\n",
      "================================================================================\n",
      "1. Computing overall performance metrics...\n",
      "\n",
      "Overall Metrics:\n",
      "  mae: $708.71\n",
      "  rmse: $1,202.27\n",
      "  mape: 13.41%\n",
      "  r2: 0.9848\n",
      "  median_error: $484.54\n",
      "  total_predictions: 558,825\n",
      "  mean_actual_price: $13,611.36\n",
      "  mean_predicted_price: $13,610.25\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2. Computing state performance...\n",
      "\n",
      "Top 15 states by volume computed\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3. Computing make performance...\n",
      "\n",
      "Top 15 makes by volume computed\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "4. Computing body type performance...\n",
      "\n",
      "Top 10 body types by volume computed\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5. Computing price range performance...\n",
      "\n",
      "7 price ranges computed\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "6. Extracting feature importance...\n",
      "\n",
      "Feature importance for 17 features extracted\n",
      "\n",
      "================================================================================\n",
      "✓ Dashboard performance data created:\n",
      "  - Overall metrics\n",
      "  - State performance (top 15)\n",
      "  - Make performance (top 15)\n",
      "  - Body type performance (top 10)\n",
      "  - Price range analysis (7 ranges)\n",
      "  - Feature importance (17 features)\n",
      "\n",
      "✓ Files saved:\n",
      "  - app/artifacts/dashboard_data.pkl\n",
      "  - app/artifacts/dashboard_data.json\n"
     ]
    }
   ],
   "source": [
    "print(\"CREATING PERFORMANCE DASHBOARD DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Overall Performance Metrics\n",
    "print(\"1. Computing overall performance metrics...\\n\")\n",
    "\n",
    "overall_metrics = {\n",
    "    'mae': float(abs(df['prediction_error']).mean()),\n",
    "    'rmse': float(np.sqrt((df['prediction_error']**2).mean())),\n",
    "    'mape': float(abs(df['error_pct']).mean()),\n",
    "    'r2': float(1 - (df['prediction_error']**2).sum() / ((df['sellingprice'] - df['sellingprice'].mean())**2).sum()),\n",
    "    'median_error': float(abs(df['prediction_error']).median()),\n",
    "    'total_predictions': len(df),\n",
    "    'mean_actual_price': float(df['sellingprice'].mean()),\n",
    "    'mean_predicted_price': float(df['predicted_price'].mean())\n",
    "}\n",
    "\n",
    "print(\"Overall Metrics:\")\n",
    "for key, value in overall_metrics.items():\n",
    "    if 'price' in key or key in ['mae', 'rmse', 'median_error']:\n",
    "        print(f\"  {key}: ${value:,.2f}\")\n",
    "    elif key in ['mape']:\n",
    "        print(f\"  {key}: {value:.2f}%\")\n",
    "    elif key == 'r2':\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value:,.0f}\")\n",
    "\n",
    "# 2. Performance by State (Top 15)\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"2. Computing state performance...\\n\")\n",
    "\n",
    "state_performance = df.groupby('state').agg({\n",
    "    'prediction_error': lambda x: abs(x).mean(),\n",
    "    'error_pct': lambda x: abs(x).mean(),\n",
    "    'sellingprice': ['count', 'mean']\n",
    "}).round(2)\n",
    "state_performance.columns = ['mae', 'mape', 'count', 'avg_price']\n",
    "state_performance = state_performance.sort_values('count', ascending=False).head(15)\n",
    "\n",
    "print(f\"Top 15 states by volume computed\")\n",
    "\n",
    "# 3. Performance by Make (Top 15)\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"3. Computing make performance...\\n\")\n",
    "\n",
    "make_performance = df.groupby('make').agg({\n",
    "    'prediction_error': lambda x: abs(x).mean(),\n",
    "    'error_pct': lambda x: abs(x).mean(),\n",
    "    'sellingprice': ['count', 'mean']\n",
    "}).round(2)\n",
    "make_performance.columns = ['mae', 'mape', 'count', 'avg_price']\n",
    "make_performance = make_performance.sort_values('count', ascending=False).head(15)\n",
    "\n",
    "print(f\"Top 15 makes by volume computed\")\n",
    "\n",
    "# 4. Performance by Body Type\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"4. Computing body type performance...\\n\")\n",
    "\n",
    "body_performance = df.groupby('body').agg({\n",
    "    'prediction_error': lambda x: abs(x).mean(),\n",
    "    'error_pct': lambda x: abs(x).mean(),\n",
    "    'sellingprice': ['count', 'mean']\n",
    "}).round(2)\n",
    "body_performance.columns = ['mae', 'mape', 'count', 'avg_price']\n",
    "body_performance = body_performance.sort_values('count', ascending=False).head(10)\n",
    "\n",
    "print(f\"Top 10 body types by volume computed\")\n",
    "\n",
    "# 5. Performance by Price Range\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"5. Computing price range performance...\\n\")\n",
    "\n",
    "price_bins = [0, 5000, 10000, 15000, 20000, 30000, 50000, 300000]\n",
    "df['price_range'] = pd.cut(df['sellingprice'], bins=price_bins, \n",
    "                            labels=['<$5k', '$5-10k', '$10-15k', '$15-20k', \n",
    "                                   '$20-30k', '$30-50k', '>$50k'])\n",
    "\n",
    "price_range_performance = df.groupby('price_range').agg({\n",
    "    'prediction_error': lambda x: abs(x).mean(),\n",
    "    'error_pct': lambda x: abs(x).mean(),\n",
    "    'sellingprice': 'count'\n",
    "}).round(2)\n",
    "price_range_performance.columns = ['mae', 'mape', 'count']\n",
    "\n",
    "print(f\"7 price ranges computed\")\n",
    "\n",
    "# 6. Feature Importance (from model)\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"6. Extracting feature importance...\\n\")\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"Feature importance for {len(features)} features extracted\")\n",
    "\n",
    "# 7. Package all dashboard data\n",
    "dashboard_data = {\n",
    "    'overall_metrics': overall_metrics,\n",
    "    'state_performance': state_performance.to_dict('index'),\n",
    "    'make_performance': make_performance.to_dict('index'),\n",
    "    'body_performance': body_performance.to_dict('index'),\n",
    "    'price_range_performance': price_range_performance.to_dict('index'),\n",
    "    'feature_importance': feature_importance.to_dict('records'),\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "# Save dashboard data\n",
    "with open('app/artifacts/dashboard_data.pkl', 'wb') as f:\n",
    "    pickle.dump(dashboard_data, f)\n",
    "\n",
    "# Also save simplified JSON version\n",
    "dashboard_json = {\n",
    "    'overall_metrics': overall_metrics,\n",
    "    'feature_importance_top10': feature_importance.head(10).to_dict('records'),\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "with open('app/artifacts/dashboard_data.json', 'w') as f:\n",
    "    json.dump(dashboard_json, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ Dashboard performance data created:\")\n",
    "print(\"  - Overall metrics\")\n",
    "print(\"  - State performance (top 15)\")\n",
    "print(\"  - Make performance (top 15)\")\n",
    "print(\"  - Body type performance (top 10)\")\n",
    "print(\"  - Price range analysis (7 ranges)\")\n",
    "print(\"  - Feature importance (17 features)\")\n",
    "print(\"\\n✓ Files saved:\")\n",
    "print(\"  - app/artifacts/dashboard_data.pkl\")\n",
    "print(\"  - app/artifacts/dashboard_data.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3d4cd0",
   "metadata": {},
   "source": [
    "## Step 5: Create Prediction Interface & Helper Functions\n",
    "\n",
    "Build simplified prediction interface for dashboard:\n",
    "- Clean prediction function with input validation\n",
    "- Feature engineering helper\n",
    "- Encoding helper\n",
    "- User-friendly error messages\n",
    "- Example usage documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32eab2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATING PREDICTION INTERFACE\n",
      "================================================================================\n",
      "✓ Prediction interface created: app/predictor.py\n",
      "\n",
      "Copying artifacts to app directory...\n",
      "  ✓ Model copied\n",
      "  ✓ Label encoders copied\n",
      "  ✓ Metadata copied\n",
      "\n",
      "================================================================================\n",
      "✓ Prediction interface ready!\n",
      "\n",
      "Files created:\n",
      "  - app/predictor.py (prediction class)\n",
      "  - app/artifacts/xgboost_optimized.pkl\n",
      "  - app/artifacts/label_encoders.pkl\n",
      "  - app/artifacts/model_metadata.pkl\n"
     ]
    }
   ],
   "source": [
    "print(\"CREATING PREDICTION INTERFACE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create prediction helper class\n",
    "prediction_code = '''\n",
    "\"\"\"\n",
    "Vehicle Price Prediction Interface\n",
    "Simplified interface for Streamlit dashboard\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "class VehiclePricePredictor:\n",
    "    \"\"\"\n",
    "    Simplified interface for vehicle price prediction in dashboard.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path='artifacts/xgboost_optimized.pkl', \n",
    "                 encoders_path='artifacts/label_encoders.pkl',\n",
    "                 metadata_path='artifacts/model_metadata.pkl'):\n",
    "        \"\"\"Load model and preprocessing artifacts.\"\"\"\n",
    "        \n",
    "        with open(model_path, 'rb') as f:\n",
    "            self.model = pickle.load(f)\n",
    "        \n",
    "        with open(encoders_path, 'rb') as f:\n",
    "            self.label_encoders = pickle.load(f)\n",
    "        \n",
    "        with open(metadata_path, 'rb') as f:\n",
    "            self.metadata = pickle.load(f)\n",
    "        \n",
    "        self.features = self.metadata['features']\n",
    "        self.numeric_features = self.metadata['numeric_features']\n",
    "        self.categorical_features = self.metadata['categorical_features']\n",
    "    \n",
    "    def validate_input(self, input_data):\n",
    "        \"\"\"Validate input data.\"\"\"\n",
    "        errors = []\n",
    "        \n",
    "        # Check required fields\n",
    "        required_fields = ['year', 'make', 'body', 'transmission', 'state', \n",
    "                          'condition', 'odometer', 'color', 'interior', \n",
    "                          'seller_grouped', 'model_grouped', 'trim_grouped', 'mmr']\n",
    "        \n",
    "        for field in required_fields:\n",
    "            if field not in input_data:\n",
    "                errors.append(f\"Missing required field: {field}\")\n",
    "        \n",
    "        if errors:\n",
    "            return False, errors\n",
    "        \n",
    "        # Validate ranges\n",
    "        if input_data.get('year', 0) < 1980 or input_data.get('year', 0) > 2025:\n",
    "            errors.append(\"Year must be between 1980 and 2025\")\n",
    "        \n",
    "        if input_data.get('odometer', 0) < 0 or input_data.get('odometer', 0) > 500000:\n",
    "            errors.append(\"Odometer must be between 0 and 500,000\")\n",
    "        \n",
    "        if input_data.get('condition', 0) < 1 or input_data.get('condition', 0) > 49:\n",
    "            errors.append(\"Condition must be between 1 and 49\")\n",
    "        \n",
    "        return len(errors) == 0, errors\n",
    "    \n",
    "    def engineer_features(self, input_data):\n",
    "        \"\"\"Create engineered features.\"\"\"\n",
    "        \n",
    "        # Vehicle age (reference year 2015)\n",
    "        input_data['vehicle_age'] = 2015 - input_data['year']\n",
    "        \n",
    "        # Log odometer\n",
    "        input_data['log_odometer'] = np.log1p(input_data['odometer'])\n",
    "        \n",
    "        # Age-odometer interaction\n",
    "        input_data['age_odo_interaction'] = input_data['vehicle_age'] * input_data['odometer'] / 10000\n",
    "        \n",
    "        # Has date flag (always 1 for dashboard predictions)\n",
    "        input_data['has_date'] = 1\n",
    "        \n",
    "        return input_data\n",
    "    \n",
    "    def encode_features(self, input_data):\n",
    "        \"\"\"Encode categorical features.\"\"\"\n",
    "        \n",
    "        encoded_data = input_data.copy()\n",
    "        \n",
    "        for feature in self.categorical_features:\n",
    "            if feature in encoded_data:\n",
    "                try:\n",
    "                    value = str(encoded_data[feature])\n",
    "                    encoded_data[feature] = self.label_encoders[feature].transform([value])[0]\n",
    "                except:\n",
    "                    # Use most common value if encoding fails\n",
    "                    encoded_data[feature] = 0\n",
    "        \n",
    "        return encoded_data\n",
    "    \n",
    "    def predict(self, input_data):\n",
    "        \"\"\"\n",
    "        Make price prediction.\n",
    "        \n",
    "        Args:\n",
    "            input_data (dict): Vehicle attributes\n",
    "        \n",
    "        Returns:\n",
    "            dict: Prediction result with confidence info\n",
    "        \"\"\"\n",
    "        \n",
    "        # Validate input\n",
    "        valid, errors = self.validate_input(input_data)\n",
    "        if not valid:\n",
    "            return {'success': False, 'errors': errors}\n",
    "        \n",
    "        # Engineer features\n",
    "        input_data = self.engineer_features(input_data)\n",
    "        \n",
    "        # Encode categorical features\n",
    "        encoded_data = self.encode_features(input_data)\n",
    "        \n",
    "        # Create feature vector in correct order\n",
    "        feature_vector = pd.DataFrame([encoded_data])[self.features]\n",
    "        \n",
    "        # Predict\n",
    "        prediction = self.model.predict(feature_vector)[0]\n",
    "        \n",
    "        # Calculate confidence based on similar vehicles\n",
    "        # (simplified - in production would use more sophisticated method)\n",
    "        confidence = 'High' if 5000 <= prediction <= 50000 else 'Medium'\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'predicted_price': float(prediction),\n",
    "            'confidence': confidence,\n",
    "            'input_summary': {\n",
    "                'vehicle': f\"{input_data['year']} {input_data['make']} {input_data['model_grouped']}\",\n",
    "                'odometer': f\"{input_data['odometer']:,} miles\",\n",
    "                'condition': input_data['condition']\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def predict_batch(self, input_list):\n",
    "        \"\"\"Predict for multiple vehicles.\"\"\"\n",
    "        results = []\n",
    "        for input_data in input_list:\n",
    "            results.append(self.predict(input_data))\n",
    "        return results\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    predictor = VehiclePricePredictor()\n",
    "    \n",
    "    example_vehicle = {\n",
    "        'year': 2012,\n",
    "        'make': 'Toyota',\n",
    "        'model_grouped': 'Camry',\n",
    "        'body': 'Sedan',\n",
    "        'transmission': 'Automatic',\n",
    "        'odometer': 50000,\n",
    "        'condition': 35,\n",
    "        'state': 'Ca',\n",
    "        'color': 'Black',\n",
    "        'interior': 'Black',\n",
    "        'seller_grouped': 'Other_Seller',\n",
    "        'trim_grouped': 'Se',\n",
    "        'mmr': 12000\n",
    "    }\n",
    "    \n",
    "    result = predictor.predict(example_vehicle)\n",
    "    print(f\"Prediction: ${result['predicted_price']:,.2f}\")\n",
    "'''\n",
    "\n",
    "# Save prediction interface\n",
    "with open('app/predictor.py', 'w') as f:\n",
    "    f.write(prediction_code)\n",
    "\n",
    "print(\"✓ Prediction interface created: app/predictor.py\")\n",
    "\n",
    "# Copy necessary artifacts to app directory\n",
    "import shutil\n",
    "\n",
    "print(\"\\nCopying artifacts to app directory...\")\n",
    "\n",
    "# Copy model\n",
    "shutil.copy('models/final/xgboost_optimized.pkl', 'app/artifacts/xgboost_optimized.pkl')\n",
    "print(\"  ✓ Model copied\")\n",
    "\n",
    "# Copy encoders\n",
    "shutil.copy('models/preprocessing/label_encoders.pkl', 'app/artifacts/label_encoders.pkl')\n",
    "print(\"  ✓ Label encoders copied\")\n",
    "\n",
    "# Copy metadata\n",
    "shutil.copy('models/final/model_metadata.pkl', 'app/artifacts/model_metadata.pkl')\n",
    "print(\"  ✓ Metadata copied\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ Prediction interface ready!\")\n",
    "print(\"\\nFiles created:\")\n",
    "print(\"  - app/predictor.py (prediction class)\")\n",
    "print(\"  - app/artifacts/xgboost_optimized.pkl\")\n",
    "print(\"  - app/artifacts/label_encoders.pkl\")\n",
    "print(\"  - app/artifacts/model_metadata.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7ef470",
   "metadata": {},
   "source": [
    "## Step 6: Create Deployment Documentation & Summary\n",
    "\n",
    "Generate complete deployment package:\n",
    "- README with usage instructions\n",
    "- Requirements file for dependencies\n",
    "- Deployment checklist\n",
    "- API documentation\n",
    "- Complete artifact inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ab73a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATING DEPLOYMENT DOCUMENTATION\n",
      "================================================================================\n",
      "✓ README.md created\n",
      "✓ requirements.txt created\n",
      "✓ deployment_summary.json created\n",
      "\n",
      "================================================================================\n",
      "COMPLETE ARTIFACT INVENTORY\n",
      "================================================================================\n",
      "\n",
      "Total artifacts: 13\n",
      "\n",
      "File                                                     Size     Type\n",
      "----------------------------------------------------------------------\n",
      "app\\artifacts\\xgboost_optimized.pkl                39808.74 KB      pkl\n",
      "app\\artifacts\\lookup_tables.pkl                       23.70 KB      pkl\n",
      "app\\artifacts\\lookup_tables.json                      15.30 KB     json\n",
      "app\\artifacts\\label_encoders.pkl                       8.44 KB      pkl\n",
      "app\\predictor.py                                       5.64 KB       py\n",
      "app\\README.md                                          3.51 KB       md\n",
      "app\\artifacts\\dashboard_data.pkl                       3.12 KB      pkl\n",
      "app\\artifacts\\sample_predictions.json                  2.72 KB     json\n",
      "app\\artifacts\\dashboard_data.json                      1.25 KB     json\n",
      "app\\artifacts\\sample_predictions.pkl                   1.15 KB      pkl\n",
      "app\\artifacts\\model_metadata.pkl                       1.13 KB      pkl\n",
      "app\\deployment_summary.json                            0.86 KB     json\n",
      "app\\requirements.txt                                   0.21 KB      txt\n",
      "----------------------------------------------------------------------\n",
      "TOTAL                                              39875.77 KB\n",
      "\n",
      "✓ artifacts_inventory.json created\n",
      "\n",
      "================================================================================\n",
      "NOTEBOOK 04 COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "Deployment Package Ready:\n",
      "  Location: app/\n",
      "  Total size: 38.94 MB\n",
      "  Total files: 13\n",
      "\n",
      "Key files created:\n",
      "  ✓ README.md - Complete documentation\n",
      "  ✓ requirements.txt - Python dependencies\n",
      "  ✓ predictor.py - Prediction interface\n",
      "  ✓ deployment_summary.json - Package metadata\n",
      "  ✓ artifacts_inventory.json - File manifest\n",
      "\n",
      "Next Steps:\n",
      "  1. Create app.py (Streamlit dashboard)\n",
      "  2. Test prediction interface\n",
      "  3. Deploy to production\n",
      "\n",
      "All 4 Core Notebooks Complete!\n",
      "Optional: Notebook 05 - Monitoring + Drift Simulation\n"
     ]
    }
   ],
   "source": [
    "print(\"CREATING DEPLOYMENT DOCUMENTATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Create README for dashboard\n",
    "readme_content = \"\"\"# Vehicle Price Prediction Dashboard\n",
    "\n",
    "## Overview\n",
    "Production-ready Streamlit dashboard for vehicle price prediction using XGBoost model.\n",
    "\n",
    "**Model Performance:**\n",
    "- MAE: $887.48\n",
    "- R-squared: 0.9682\n",
    "- MAPE: 12.30%\n",
    "\n",
    "## Quick Start\n",
    "```bash\n",
    "# Install dependencies\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# Run dashboard\n",
    "streamlit run app.py\n",
    "```\n",
    "\n",
    "## Dashboard Features\n",
    "\n",
    "### 1. Price Prediction Tool\n",
    "- Interactive form with dropdowns for vehicle attributes\n",
    "- Real-time price predictions\n",
    "- Confidence scoring\n",
    "- MMR comparison\n",
    "\n",
    "### 2. Model Insights\n",
    "- Feature importance visualization\n",
    "- Performance metrics dashboard\n",
    "- Prediction confidence analysis\n",
    "\n",
    "### 3. Market Analysis\n",
    "- Price trends by segment (state, make, body type)\n",
    "- Popular vehicle combinations\n",
    "- Market statistics\n",
    "\n",
    "### 4. Sample Predictions\n",
    "- Pre-computed examples across price ranges\n",
    "- Prediction explanations\n",
    "- Accuracy demonstrations\n",
    "\n",
    "## File Structure\n",
    "```\n",
    "app/\n",
    "├── app.py                      # Main Streamlit dashboard\n",
    "├── predictor.py                # Prediction interface class\n",
    "├── artifacts/\n",
    "│   ├── xgboost_optimized.pkl   # Trained model\n",
    "│   ├── label_encoders.pkl      # Categorical encoders\n",
    "│   ├── model_metadata.pkl      # Model info\n",
    "│   ├── lookup_tables.pkl       # Dropdown options\n",
    "│   ├── lookup_tables.json      # Simplified lookups\n",
    "│   ├── sample_predictions.pkl  # Example predictions\n",
    "│   ├── sample_predictions.json\n",
    "│   ├── dashboard_data.pkl      # Performance metrics\n",
    "│   └── dashboard_data.json\n",
    "├── assets/                     # Images, logos\n",
    "└── requirements.txt            # Python dependencies\n",
    "```\n",
    "\n",
    "## Usage Example\n",
    "```python\n",
    "from predictor import VehiclePricePredictor\n",
    "\n",
    "# Initialize predictor\n",
    "predictor = VehiclePricePredictor()\n",
    "\n",
    "# Make prediction\n",
    "vehicle = {\n",
    "    'year': 2012,\n",
    "    'make': 'Toyota',\n",
    "    'model_grouped': 'Camry',\n",
    "    'body': 'Sedan',\n",
    "    'transmission': 'Automatic',\n",
    "    'odometer': 50000,\n",
    "    'condition': 35,\n",
    "    'state': 'Ca',\n",
    "    'color': 'Black',\n",
    "    'interior': 'Black',\n",
    "    'seller_grouped': 'Other_Seller',\n",
    "    'trim_grouped': 'Se',\n",
    "    'mmr': 12000\n",
    "}\n",
    "\n",
    "result = predictor.predict(vehicle)\n",
    "print(f\"Predicted Price: ${result['predicted_price']:,.2f}\")\n",
    "```\n",
    "\n",
    "## Model Details\n",
    "\n",
    "**Algorithm:** XGBoost Regressor (Optimized with Optuna)\n",
    "\n",
    "**Features (17 total):**\n",
    "- Numeric: year, condition, odometer, mmr, vehicle_age, log_odometer, age_odo_interaction, has_date\n",
    "- Categorical: make, body, transmission, state, color, interior, seller_grouped, model_grouped, trim_grouped\n",
    "\n",
    "**Training Data:** 558,825 vehicle sales records (2014-2015)\n",
    "\n",
    "**Key Feature Importance:**\n",
    "1. MMR (61.7%)\n",
    "2. Body Type (9.0%)\n",
    "3. Age-Odometer Interaction (8.5%)\n",
    "4. Make (5.9%)\n",
    "5. Model (3.0%)\n",
    "\n",
    "## Performance by Segment\n",
    "\n",
    "- **Best Accuracy:** $30k-$50k range (3.19% MAPE)\n",
    "- **Best State:** PA (7.05% MAPE)\n",
    "- **Best Make:** BMW (7.61% MAPE)\n",
    "\n",
    "## Deployment Checklist\n",
    "\n",
    "- [x] Model trained and optimized\n",
    "- [x] Artifacts packaged\n",
    "- [x] Prediction interface created\n",
    "- [x] Sample data prepared\n",
    "- [x] Documentation complete\n",
    "- [ ] Streamlit app.py created\n",
    "- [ ] Testing completed\n",
    "- [ ] Production deployment\n",
    "\n",
    "## Support\n",
    "\n",
    "For issues or questions, refer to the project notebooks:\n",
    "- Notebook 00: Data Overview\n",
    "- Notebook 01: Data Cleaning\n",
    "- Notebook 02: Modeling\n",
    "- Notebook 03: Explainability\n",
    "- Notebook 04: Dashboard Prep (this notebook)\n",
    "\n",
    "## License\n",
    "\n",
    "Internal use only - Vehicle Sales & Market Insights Project\n",
    "\"\"\"\n",
    "\n",
    "with open('app/README.md', 'w', encoding='utf-8') as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(\"✓ README.md created\")\n",
    "\n",
    "# 2. Create requirements.txt\n",
    "requirements = \"\"\"# Core Dependencies\n",
    "streamlit==1.31.0\n",
    "pandas==2.2.0\n",
    "numpy==1.26.4\n",
    "xgboost==2.0.3\n",
    "scikit-learn==1.4.0\n",
    "\n",
    "# Visualization\n",
    "matplotlib==3.8.2\n",
    "seaborn==0.13.2\n",
    "plotly==5.18.0\n",
    "\n",
    "# Utilities\n",
    "pickle5==0.0.12\n",
    "\"\"\"\n",
    "\n",
    "with open('app/requirements.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(requirements)\n",
    "\n",
    "print(\"✓ requirements.txt created\")\n",
    "\n",
    "# 3. Create deployment summary\n",
    "deployment_summary = {\n",
    "    'project': 'Vehicle Sales & Market Insights',\n",
    "    'model': 'XGBoost Regressor (Optimized)',\n",
    "    'created_date': datetime.now().isoformat(),\n",
    "    'performance': {\n",
    "        'test_mae': metadata['performance']['test_mae'],\n",
    "        'test_r2': metadata['performance']['test_r2'],\n",
    "        'test_mape': metadata['performance']['test_mape']\n",
    "    },\n",
    "    'artifacts': {\n",
    "        'model': 'app/artifacts/xgboost_optimized.pkl',\n",
    "        'encoders': 'app/artifacts/label_encoders.pkl',\n",
    "        'metadata': 'app/artifacts/model_metadata.pkl',\n",
    "        'lookups': 'app/artifacts/lookup_tables.pkl',\n",
    "        'samples': 'app/artifacts/sample_predictions.pkl',\n",
    "        'dashboard_data': 'app/artifacts/dashboard_data.pkl'\n",
    "    },\n",
    "    'features': {\n",
    "        'total': len(metadata['features']),\n",
    "        'numeric': len(metadata['numeric_features']),\n",
    "        'categorical': len(metadata['categorical_features'])\n",
    "    },\n",
    "    'data': {\n",
    "        'training_samples': metadata['n_train_samples'],\n",
    "        'validation_samples': metadata['n_val_samples'],\n",
    "        'test_samples': metadata['n_test_samples'],\n",
    "        'total_samples': 558825\n",
    "    },\n",
    "    'deployment_ready': True\n",
    "}\n",
    "\n",
    "with open('app/deployment_summary.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(deployment_summary, f, indent=2)\n",
    "\n",
    "print(\"✓ deployment_summary.json created\")\n",
    "\n",
    "# 4. List all artifacts\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPLETE ARTIFACT INVENTORY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "artifacts_inventory = []\n",
    "\n",
    "# Walk through app directory\n",
    "for root, dirs, files in os.walk('app'):\n",
    "    for file in files:\n",
    "        filepath = os.path.join(root, file)\n",
    "        filesize = os.path.getsize(filepath) / 1024  # KB\n",
    "        artifacts_inventory.append({\n",
    "            'file': filepath,\n",
    "            'size_kb': round(filesize, 2),\n",
    "            'type': file.split('.')[-1]\n",
    "        })\n",
    "\n",
    "artifacts_inventory.sort(key=lambda x: x['size_kb'], reverse=True)\n",
    "\n",
    "print(f\"\\nTotal artifacts: {len(artifacts_inventory)}\\n\")\n",
    "print(f\"{'File':<50} {'Size':>10} {'Type':>8}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "total_size = 0\n",
    "for artifact in artifacts_inventory:\n",
    "    print(f\"{artifact['file']:<50} {artifact['size_kb']:>8.2f} KB {artifact['type']:>8}\")\n",
    "    total_size += artifact['size_kb']\n",
    "\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'TOTAL':<50} {total_size:>8.2f} KB\")\n",
    "\n",
    "# Save inventory\n",
    "with open('app/artifacts_inventory.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(artifacts_inventory, f, indent=2)\n",
    "\n",
    "print(\"\\n✓ artifacts_inventory.json created\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"NOTEBOOK 04 COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nDeployment Package Ready:\")\n",
    "print(f\"  Location: app/\")\n",
    "print(f\"  Total size: {total_size/1024:.2f} MB\")\n",
    "print(f\"  Total files: {len(artifacts_inventory)}\")\n",
    "print(\"\\nKey files created:\")\n",
    "print(\"  ✓ README.md - Complete documentation\")\n",
    "print(\"  ✓ requirements.txt - Python dependencies\")\n",
    "print(\"  ✓ predictor.py - Prediction interface\")\n",
    "print(\"  ✓ deployment_summary.json - Package metadata\")\n",
    "print(\"  ✓ artifacts_inventory.json - File manifest\")\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"  1. Create app.py (Streamlit dashboard)\")\n",
    "print(\"  2. Test prediction interface\")\n",
    "print(\"  3. Deploy to production\")\n",
    "print(\"\\nAll 4 Core Notebooks Complete!\")\n",
    "print(\"Optional: Notebook 05 - Monitoring + Drift Simulation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888d2298",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
