{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54170a3e",
   "metadata": {},
   "source": [
    "# Notebook 01: Data Cleaning + Preprocessing Pipeline\n",
    "\n",
    "**Project:** Vehicle Sales & Market Insights  \n",
    "**Dataset:** car_prices_interim.csv (from Notebook 00)  \n",
    "**Purpose:** Clean data quality issues and build reusable preprocessing pipeline\n",
    "\n",
    "## Objective\n",
    "Address data quality issues identified in Notebook 00 and create production-ready cleaned dataset:\n",
    "- Standardize categorical variables\n",
    "- Handle outliers\n",
    "- Impute missing values strategically\n",
    "- Engineer essential features\n",
    "- Save preprocessed data for modeling\n",
    "\n",
    "## Approach\n",
    "We will NOT drop records with missing temporal data. Instead, we'll use a has_date flag to preserve all 558,837 records, maximizing training data while handling missing patterns appropriately.\n",
    "\n",
    "## Step 1: Environment Setup\n",
    "Load necessary libraries and the interim dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c43b9491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Setup Complete\n",
      "Pandas: 2.3.3\n",
      "NumPy: 1.26.4\n",
      "\n",
      "Dataset loaded: (558837, 23)\n",
      "Memory: 432.5 MB\n",
      "\n",
      "Columns: ['year', 'make', 'model', 'trim', 'body', 'transmission', 'vin', 'state', 'condition', 'odometer', 'color', 'interior', 'seller', 'mmr', 'sellingprice', 'saledate', 'sale_year', 'sale_month', 'sale_day', 'sale_dayofweek', 'sale_quarter', 'vehicle_age', 'price_diff']\n"
     ]
    }
   ],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Preprocessing & pipelines\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Utilities\n",
    "import pickle\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Environment Setup Complete\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "\n",
    "# Load interim data from Notebook 00\n",
    "df = pd.read_csv('data/interim/car_prices_interim.csv')\n",
    "\n",
    "print(f\"\\nDataset loaded: {df.shape}\")\n",
    "print(f\"Memory: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11479a14",
   "metadata": {},
   "source": [
    "## Step 2: Baseline Data Quality Assessment\n",
    "\n",
    "Review data quality issues identified in Notebook 00 to establish cleaning priorities.\n",
    "Document current state before applying transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb42bb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASELINE DATA QUALITY ASSESSMENT\n",
      "================================================================================\n",
      "1. Missing Values:\n",
      "                Missing  Percent\n",
      "sale_dayofweek   163348    29.23\n",
      "vehicle_age      163348    29.23\n",
      "sale_quarter     163348    29.23\n",
      "saledate         163348    29.23\n",
      "sale_year        163348    29.23\n",
      "sale_month       163348    29.23\n",
      "sale_day         163348    29.23\n",
      "transmission      65352    11.69\n",
      "body              13195     2.36\n",
      "condition         11820     2.12\n",
      "trim              10651     1.91\n",
      "model             10399     1.86\n",
      "make              10301     1.84\n",
      "color               749     0.13\n",
      "interior            749     0.13\n",
      "odometer             94     0.02\n",
      "mmr                  38     0.01\n",
      "price_diff           38     0.01\n",
      "sellingprice         12     0.00\n",
      "vin                   4     0.00\n",
      "\n",
      "2. Known Data Quality Issues:\n",
      "   - Body type unique values: 87 (capitalization inconsistency)\n",
      "   - Transmission unique values: 4\n",
      "   - Invalid transmission entries:\n",
      "transmission\n",
      "     sedan    15\n",
      "     Sedan    11\n",
      "   - Odometer outliers (>500k): 81\n",
      "   - Duplicate VINs: 8539\n",
      "\n",
      "3. Target Variable (sellingprice):\n",
      "   - Missing: 12\n",
      "   - Range: $1 to $230000\n",
      "   - Mean: $13611.36\n",
      "\n",
      "4. Temporal Features:\n",
      "   - Records with valid dates: 395,489 (70.8%)\n",
      "   - Records without dates: 163,348 (29.2%)\n",
      "\n",
      "================================================================================\n",
      "Data Cleaning Plan:\n",
      "  1. Remove records with missing target (sellingprice)\n",
      "  2. Standardize categorical variables (fix capitalization)\n",
      "  3. Fix invalid transmission values\n",
      "  4. Handle odometer outliers (cap at 500k)\n",
      "  5. Create has_date flag, drop derived temporal features\n",
      "  6. Impute remaining missing values\n"
     ]
    }
   ],
   "source": [
    "print(\"BASELINE DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Missing values\n",
    "print(\"1. Missing Values:\")\n",
    "missing = df.isnull().sum()\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing': missing[missing > 0],\n",
    "    'Percent': (missing[missing > 0] / len(df) * 100).round(2)\n",
    "}).sort_values('Percent', ascending=False)\n",
    "print(missing_df)\n",
    "\n",
    "# Known quality issues from Notebook 00\n",
    "print(\"\\n2. Known Data Quality Issues:\")\n",
    "print(f\"   - Body type unique values: {df['body'].nunique()} (capitalization inconsistency)\")\n",
    "print(f\"   - Transmission unique values: {df['transmission'].nunique()}\")\n",
    "\n",
    "# Check invalid transmission\n",
    "invalid_trans = df[~df['transmission'].isin(['automatic', 'manual'])]['transmission'].value_counts()\n",
    "if len(invalid_trans) > 0:\n",
    "    print(f\"   - Invalid transmission entries:\")\n",
    "    print(invalid_trans.to_string().replace('\\n', '\\n     '))\n",
    "\n",
    "print(f\"   - Odometer outliers (>500k): {(df['odometer'] > 500000).sum()}\")\n",
    "print(f\"   - Duplicate VINs: {df['vin'].duplicated().sum()}\")\n",
    "\n",
    "# Target variable\n",
    "print(\"\\n3. Target Variable (sellingprice):\")\n",
    "print(f\"   - Missing: {df['sellingprice'].isnull().sum()}\")\n",
    "print(f\"   - Range: ${df['sellingprice'].min():.0f} to ${df['sellingprice'].max():.0f}\")\n",
    "print(f\"   - Mean: ${df['sellingprice'].mean():.2f}\")\n",
    "\n",
    "print(\"\\n4. Temporal Features:\")\n",
    "print(f\"   - Records with valid dates: {df['saledate'].notna().sum():,} ({df['saledate'].notna().sum()/len(df)*100:.1f}%)\")\n",
    "print(f\"   - Records without dates: {df['saledate'].isna().sum():,} ({df['saledate'].isna().sum()/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Data Cleaning Plan:\")\n",
    "print(\"  1. Remove records with missing target (sellingprice)\")\n",
    "print(\"  2. Standardize categorical variables (fix capitalization)\")\n",
    "print(\"  3. Fix invalid transmission values\")\n",
    "print(\"  4. Handle odometer outliers (cap at 500k)\")\n",
    "print(\"  5. Create has_date flag, drop derived temporal features\")\n",
    "print(\"  6. Impute remaining missing values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343e5bee",
   "metadata": {},
   "source": [
    "## Step 3: Remove Records with Missing Target Variable\n",
    "\n",
    "Remove records where sellingprice is missing (12 records).\n",
    "These cannot be used for model training and represent only 0.002% of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "05bda28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REMOVING RECORDS WITH MISSING TARGET\n",
      "================================================================================\n",
      "Before: 558,837 records\n",
      "Missing sellingprice: 12\n",
      "\n",
      "After: 558,825 records\n",
      "Removed: 12 records (0.002%)\n",
      "Retention rate: 99.998%\n",
      "\n",
      "Verification:\n",
      "Missing sellingprice: 0\n",
      "Mean: $13611.36\n",
      "Median: $12100.00\n"
     ]
    }
   ],
   "source": [
    "print(\"REMOVING RECORDS WITH MISSING TARGET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"Before: {len(df):,} records\")\n",
    "print(f\"Missing sellingprice: {df['sellingprice'].isnull().sum()}\")\n",
    "\n",
    "# Remove records with missing target\n",
    "df = df[df['sellingprice'].notna()].copy()\n",
    "\n",
    "print(f\"\\nAfter: {len(df):,} records\")\n",
    "print(f\"Removed: 12 records (0.002%)\")\n",
    "print(f\"Retention rate: 99.998%\")\n",
    "\n",
    "# Verify\n",
    "print(f\"\\nVerification:\")\n",
    "print(f\"Missing sellingprice: {df['sellingprice'].isnull().sum()}\")\n",
    "print(f\"Mean: ${df['sellingprice'].mean():.2f}\")\n",
    "print(f\"Median: ${df['sellingprice'].median():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b65a3fd",
   "metadata": {},
   "source": [
    "## Step 4: Standardize Categorical Variables\n",
    "\n",
    "Fix inconsistent capitalization in categorical text fields.\n",
    "Convert to title case for consistency and reduce cardinality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "16533bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CATEGORICAL VARIABLE STANDARDIZATION\n",
      "================================================================================\n",
      "Before Standardization:\n",
      "  make: 96 unique values\n",
      "  model: 973 unique values\n",
      "  body: 87 unique values\n",
      "  transmission: 4 unique values\n",
      "  color: 46 unique values\n",
      "  interior: 17 unique values\n",
      "  state: 64 unique values\n",
      "\n",
      "After Standardization:\n",
      "  make: 67 unique values\n",
      "  model: 852 unique values\n",
      "  body: 47 unique values\n",
      "  transmission: 4 unique values\n",
      "  color: 47 unique values\n",
      "  interior: 18 unique values\n",
      "  state: 64 unique values\n",
      "\n",
      "Body Type - Top 10:\n",
      "body\n",
      "Sedan          241332\n",
      "Suv            143844\n",
      "Hatchback       26237\n",
      "Minivan         25529\n",
      "Coupe           17752\n",
      "Crew Cab        16394\n",
      "Wagon           16128\n",
      "Nan             13195\n",
      "Convertible     10476\n",
      "Supercrew        9033\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Transmission Values:\n",
      "transmission\n",
      "Automatic    475904\n",
      "Nan           65351\n",
      "Manual        17544\n",
      "Sedan            26\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Checking for 'Nan' string conversions:\n",
      "  make: 10,301 'Nan' strings\n",
      "  model: 10,399 'Nan' strings\n",
      "  body: 13,195 'Nan' strings\n",
      "  transmission: 65,351 'Nan' strings\n",
      "  color: 749 'Nan' strings\n",
      "  interior: 749 'Nan' strings\n"
     ]
    }
   ],
   "source": [
    "print(\"CATEGORICAL VARIABLE STANDARDIZATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Categorical columns to standardize\n",
    "cat_columns = ['make', 'model', 'body', 'transmission', 'color', 'interior', 'state']\n",
    "\n",
    "print(\"Before Standardization:\")\n",
    "for col in cat_columns:\n",
    "    print(f\"  {col}: {df[col].nunique()} unique values\")\n",
    "\n",
    "# Apply title case standardization\n",
    "for col in cat_columns:\n",
    "    df[col] = df[col].astype(str).str.strip().str.title()\n",
    "\n",
    "print(\"\\nAfter Standardization:\")\n",
    "for col in cat_columns:\n",
    "    print(f\"  {col}: {df[col].nunique()} unique values\")\n",
    "\n",
    "# Check specific improvements\n",
    "print(\"\\nBody Type - Top 10:\")\n",
    "print(df['body'].value_counts().head(10))\n",
    "\n",
    "print(\"\\nTransmission Values:\")\n",
    "print(df['transmission'].value_counts())\n",
    "\n",
    "# Check for 'Nan' strings created by conversion\n",
    "print(\"\\nChecking for 'Nan' string conversions:\")\n",
    "nan_counts = {}\n",
    "for col in cat_columns:\n",
    "    nan_count = (df[col] == 'Nan').sum()\n",
    "    if nan_count > 0:\n",
    "        nan_counts[col] = nan_count\n",
    "        \n",
    "if nan_counts:\n",
    "    for col, count in nan_counts.items():\n",
    "        print(f\"  {col}: {count:,} 'Nan' strings\")\n",
    "else:\n",
    "    print(\"  No 'Nan' strings found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a299ad6",
   "metadata": {},
   "source": [
    "## Step 5: Fix Missing Value Encoding and Invalid Transmission\n",
    "\n",
    "Convert 'Nan' strings back to proper NaN values.\n",
    "Remove invalid 'Sedan' entries from transmission field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d710a54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIXING MISSING VALUES AND INVALID DATA\n",
      "================================================================================\n",
      "Converting 'Nan' strings to NaN:\n",
      "  make: 10,301 'Nan' strings → 10,301 NaN values\n",
      "  model: 10,399 'Nan' strings → 10,399 NaN values\n",
      "  body: 13,195 'Nan' strings → 13,195 NaN values\n",
      "  transmission: 65,351 'Nan' strings → 65,351 NaN values\n",
      "  color: 749 'Nan' strings → 749 NaN values\n",
      "  interior: 749 'Nan' strings → 749 NaN values\n",
      "\n",
      "Fixing Invalid Transmission Values:\n",
      "Before:\n",
      "transmission\n",
      "  Automatic    475904\n",
      "  NaN           65351\n",
      "  Manual        17544\n",
      "  Sedan            26\n",
      "\n",
      "After (converted 26 'Sedan' to NaN):\n",
      "transmission\n",
      "  Automatic    475904\n",
      "  NaN           65377\n",
      "  Manual        17544\n",
      "\n",
      "================================================================================\n",
      "Summary:\n",
      "Valid transmission values: 493,448\n",
      "Body types (excl. missing): 46\n",
      "Total records: 558,825\n"
     ]
    }
   ],
   "source": [
    "print(\"FIXING MISSING VALUES AND INVALID DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "cat_columns = ['make', 'model', 'body', 'transmission', 'color', 'interior']\n",
    "\n",
    "# Convert 'Nan' strings to actual NaN\n",
    "print(\"Converting 'Nan' strings to NaN:\")\n",
    "for col in cat_columns:\n",
    "    before = (df[col] == 'Nan').sum()\n",
    "    df[col] = df[col].replace('Nan', np.nan)\n",
    "    after = df[col].isnull().sum()\n",
    "    print(f\"  {col}: {before:,} 'Nan' strings → {after:,} NaN values\")\n",
    "\n",
    "# Fix invalid transmission values\n",
    "print(\"\\nFixing Invalid Transmission Values:\")\n",
    "print(f\"Before:\")\n",
    "print(df['transmission'].value_counts(dropna=False).to_string().replace('\\n', '\\n  '))\n",
    "\n",
    "invalid_count = (df['transmission'] == 'Sedan').sum()\n",
    "df.loc[df['transmission'] == 'Sedan', 'transmission'] = np.nan\n",
    "\n",
    "print(f\"\\nAfter (converted {invalid_count} 'Sedan' to NaN):\")\n",
    "print(df['transmission'].value_counts(dropna=False).to_string().replace('\\n', '\\n  '))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Summary:\")\n",
    "print(f\"Valid transmission values: {df['transmission'].notna().sum():,}\")\n",
    "print(f\"Body types (excl. missing): {df['body'].nunique()}\")\n",
    "print(f\"Total records: {len(df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fe626a",
   "metadata": {},
   "source": [
    "## Step 6: Handle Outliers in Numeric Features\n",
    "\n",
    "Cap extreme odometer outliers at 500,000 miles.\n",
    "Preserve price outliers as they represent legitimate luxury vehicles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9351049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTLIER HANDLING\n",
      "================================================================================\n",
      "Odometer Analysis:\n",
      "  Mean: 68,321\n",
      "  Median: 52,255\n",
      "  99th percentile: 226,670\n",
      "  Max: 999,999\n",
      "\n",
      "  Outlier counts:\n",
      "    >200k miles: 12,111\n",
      "    >300k miles: 744\n",
      "    >400k miles: 123\n",
      "    >500k miles: 81\n",
      "\n",
      "Applied cap at 500,000 miles\n",
      "  Records capped: 81\n",
      "  New max: 500,000\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Other Numeric Features (checking for extreme outliers):\n",
      "  condition: 0 outliers beyond 3×IQR\n",
      "  mmr: 3,029 outliers beyond 3×IQR\n",
      "  sellingprice: 2,927 outliers beyond 3×IQR\n",
      "\n",
      "Decision: Keep price outliers (legitimate luxury vehicles)\n",
      "          Condition scale (1-49) is valid as-is\n"
     ]
    }
   ],
   "source": [
    "print(\"OUTLIER HANDLING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Analyze odometer distribution\n",
    "print(\"Odometer Analysis:\")\n",
    "print(f\"  Mean: {df['odometer'].mean():,.0f}\")\n",
    "print(f\"  Median: {df['odometer'].median():,.0f}\")\n",
    "print(f\"  99th percentile: {df['odometer'].quantile(0.99):,.0f}\")\n",
    "print(f\"  Max: {df['odometer'].max():,.0f}\")\n",
    "\n",
    "print(\"\\n  Outlier counts:\")\n",
    "print(f\"    >200k miles: {(df['odometer'] > 200000).sum():,}\")\n",
    "print(f\"    >300k miles: {(df['odometer'] > 300000).sum():,}\")\n",
    "print(f\"    >400k miles: {(df['odometer'] > 400000).sum():,}\")\n",
    "print(f\"    >500k miles: {(df['odometer'] > 500000).sum():,}\")\n",
    "\n",
    "# Apply capping\n",
    "odometer_cap = 500000\n",
    "outliers_capped = (df['odometer'] > odometer_cap).sum()\n",
    "df['odometer'] = df['odometer'].clip(upper=odometer_cap)\n",
    "\n",
    "print(f\"\\nApplied cap at {odometer_cap:,} miles\")\n",
    "print(f\"  Records capped: {outliers_capped}\")\n",
    "print(f\"  New max: {df['odometer'].max():,.0f}\")\n",
    "\n",
    "# Check other numeric features\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Other Numeric Features (checking for extreme outliers):\")\n",
    "\n",
    "for col in ['condition', 'mmr', 'sellingprice']:\n",
    "    q1 = df[col].quantile(0.25)\n",
    "    q3 = df[col].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    outliers = ((df[col] < q1 - 3*iqr) | (df[col] > q3 + 3*iqr)).sum()\n",
    "    print(f\"  {col}: {outliers:,} outliers beyond 3×IQR\")\n",
    "\n",
    "print(\"\\nDecision: Keep price outliers (legitimate luxury vehicles)\")\n",
    "print(\"          Condition scale (1-49) is valid as-is\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b5a266",
   "metadata": {},
   "source": [
    "## Step 7: Handle Temporal Features\n",
    "\n",
    "Create has_date binary flag to capture temporal data availability.\n",
    "Drop derived temporal features since they show weak correlation with price and 29% are missing.\n",
    "This preserves all records while avoiding artificial imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df5b3ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEMPORAL FEATURE HANDLING\n",
      "================================================================================\n",
      "Records with dates: 395,489 (70.8%)\n",
      "Records without dates: 163,336 (29.2%)\n",
      "\n",
      "Price Analysis by Date Availability:\n",
      "  Mean price WITH dates:    $13,176.61\n",
      "  Mean price WITHOUT dates: $14,664.03\n",
      "  Difference: $1,487.42\n",
      "\n",
      "Dropping 7 derived temporal features:\n",
      "  - sale_year\n",
      "  - sale_month\n",
      "  - sale_day\n",
      "  - sale_dayofweek\n",
      "  - sale_quarter\n",
      "  - vehicle_age\n",
      "  - price_diff\n",
      "\n",
      "New shape: (558825, 17)\n",
      "Columns retained: 17\n",
      "All 558,825 records preserved\n"
     ]
    }
   ],
   "source": [
    "print(\"TEMPORAL FEATURE HANDLING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create has_date flag\n",
    "df['has_date'] = df['saledate'].notna().astype(int)\n",
    "\n",
    "print(f\"Records with dates: {df['has_date'].sum():,} ({df['has_date'].mean()*100:.1f}%)\")\n",
    "print(f\"Records without dates: {(1-df['has_date']).sum():,} ({(1-df['has_date'].mean())*100:.1f}%)\")\n",
    "\n",
    "# Analyze price difference\n",
    "print(f\"\\nPrice Analysis by Date Availability:\")\n",
    "print(f\"  Mean price WITH dates:    ${df[df['has_date']==1]['sellingprice'].mean():,.2f}\")\n",
    "print(f\"  Mean price WITHOUT dates: ${df[df['has_date']==0]['sellingprice'].mean():,.2f}\")\n",
    "print(f\"  Difference: ${df[df['has_date']==0]['sellingprice'].mean() - df[df['has_date']==1]['sellingprice'].mean():,.2f}\")\n",
    "\n",
    "# Drop derived temporal features\n",
    "temporal_to_drop = ['sale_year', 'sale_month', 'sale_day', 'sale_dayofweek', \n",
    "                    'sale_quarter', 'vehicle_age', 'price_diff']\n",
    "\n",
    "print(f\"\\nDropping {len(temporal_to_drop)} derived temporal features:\")\n",
    "for col in temporal_to_drop:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "df = df.drop(columns=temporal_to_drop)\n",
    "\n",
    "print(f\"\\nNew shape: {df.shape}\")\n",
    "print(f\"Columns retained: {len(df.columns)}\")\n",
    "print(f\"All {len(df):,} records preserved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7842099d",
   "metadata": {},
   "source": [
    "## Step 8: Missing Value Imputation\n",
    "\n",
    "Apply appropriate imputation strategies for remaining missing values:\n",
    "- Categorical: Mode imputation or 'Unknown' category\n",
    "- Numeric: Median imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1a7db2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISSING VALUE IMPUTATION\n",
      "================================================================================\n",
      "Before Imputation:\n",
      "              Missing  Percent\n",
      "saledate       163336    29.23\n",
      "transmission    65377    11.70\n",
      "body            13195     2.36\n",
      "condition       11820     2.12\n",
      "trim            10651     1.91\n",
      "model           10399     1.86\n",
      "make            10301     1.84\n",
      "color             749     0.13\n",
      "interior          749     0.13\n",
      "odometer           94     0.02\n",
      "vin                 4     0.00\n",
      "mmr                26     0.00\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Applying Imputation:\n",
      "\n",
      "1. Transmission: 65,377 → 'Automatic'\n",
      "2. Body: 13,195 → 'Sedan'\n",
      "3. Condition: 11,820 → 35.0\n",
      "4. Make: 10,301 → 'Unknown'\n",
      "5. Model: 10,399 → 'Unknown'\n",
      "6. Trim: 10,651 → 'Unknown'\n",
      "7. Color: 749 → 'Black'\n",
      "8. Interior: 749 → 'Black'\n",
      "9. Odometer: 94 → 52,255\n",
      "10. MMR: 26 → $12,250\n",
      "11. VIN: 4 → 'UNKNOWN_VIN'\n",
      "\n",
      "================================================================================\n",
      "After Imputation:\n",
      "Only saledate has 163,336 missing (intentionally kept)\n",
      "\n",
      "Data quality: 98.28% complete\n"
     ]
    }
   ],
   "source": [
    "print(\"MISSING VALUE IMPUTATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"Before Imputation:\")\n",
    "missing = df.isnull().sum()\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing': missing[missing > 0],\n",
    "    'Percent': (missing[missing > 0] / len(df) * 100).round(2)\n",
    "}).sort_values('Percent', ascending=False)\n",
    "print(missing_df)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Applying Imputation:\\n\")\n",
    "\n",
    "# Transmission - Mode\n",
    "transmission_mode = df['transmission'].mode()[0]\n",
    "n_trans = df['transmission'].isnull().sum()\n",
    "df['transmission'].fillna(transmission_mode, inplace=True)\n",
    "print(f\"1. Transmission: {n_trans:,} → '{transmission_mode}'\")\n",
    "\n",
    "# Body - Mode\n",
    "body_mode = df['body'].mode()[0]\n",
    "n_body = df['body'].isnull().sum()\n",
    "df['body'].fillna(body_mode, inplace=True)\n",
    "print(f\"2. Body: {n_body:,} → '{body_mode}'\")\n",
    "\n",
    "# Condition - Median\n",
    "condition_median = df['condition'].median()\n",
    "n_cond = df['condition'].isnull().sum()\n",
    "df['condition'].fillna(condition_median, inplace=True)\n",
    "print(f\"3. Condition: {n_cond:,} → {condition_median}\")\n",
    "\n",
    "# Make/Model/Trim - 'Unknown'\n",
    "n_make = df['make'].isnull().sum()\n",
    "n_model = df['model'].isnull().sum()\n",
    "n_trim = df['trim'].isnull().sum()\n",
    "df['make'].fillna('Unknown', inplace=True)\n",
    "df['model'].fillna('Unknown', inplace=True)\n",
    "df['trim'].fillna('Unknown', inplace=True)\n",
    "print(f\"4. Make: {n_make:,} → 'Unknown'\")\n",
    "print(f\"5. Model: {n_model:,} → 'Unknown'\")\n",
    "print(f\"6. Trim: {n_trim:,} → 'Unknown'\")\n",
    "\n",
    "# Color/Interior - Mode\n",
    "color_mode = df['color'].mode()[0]\n",
    "interior_mode = df['interior'].mode()[0]\n",
    "n_color = df['color'].isnull().sum()\n",
    "n_interior = df['interior'].isnull().sum()\n",
    "df['color'].fillna(color_mode, inplace=True)\n",
    "df['interior'].fillna(interior_mode, inplace=True)\n",
    "print(f\"7. Color: {n_color:,} → '{color_mode}'\")\n",
    "print(f\"8. Interior: {n_interior:,} → '{interior_mode}'\")\n",
    "\n",
    "# Odometer - Median\n",
    "n_odo = df['odometer'].isnull().sum()\n",
    "if n_odo > 0:\n",
    "    odometer_median = df['odometer'].median()\n",
    "    df['odometer'].fillna(odometer_median, inplace=True)\n",
    "    print(f\"9. Odometer: {n_odo:,} → {odometer_median:,.0f}\")\n",
    "\n",
    "# MMR - Median\n",
    "n_mmr = df['mmr'].isnull().sum()\n",
    "if n_mmr > 0:\n",
    "    mmr_median = df['mmr'].median()\n",
    "    df['mmr'].fillna(mmr_median, inplace=True)\n",
    "    print(f\"10. MMR: {n_mmr:,} → ${mmr_median:,.0f}\")\n",
    "\n",
    "# VIN - Placeholder (not used for modeling)\n",
    "n_vin = df['vin'].isnull().sum()\n",
    "if n_vin > 0:\n",
    "    df['vin'].fillna('UNKNOWN_VIN', inplace=True)\n",
    "    print(f\"11. VIN: {n_vin:,} → 'UNKNOWN_VIN'\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"After Imputation:\")\n",
    "remaining = df.isnull().sum()[df.isnull().sum() > 0]\n",
    "if len(remaining) == 0:\n",
    "    print(\"All features fully imputed!\")\n",
    "elif len(remaining) == 1 and 'saledate' in remaining.index:\n",
    "    print(f\"Only saledate has {remaining['saledate']:,} missing (intentionally kept)\")\n",
    "else:\n",
    "    print(remaining)\n",
    "\n",
    "print(f\"\\nData quality: {(1 - df.isnull().sum().sum()/(len(df)*len(df.columns)))*100:.2f}% complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a47ad88",
   "metadata": {},
   "source": [
    "## Step 9: Feature Engineering\n",
    "\n",
    "Create essential features for modeling:\n",
    "- Vehicle age from current year\n",
    "- Log-transformed odometer (handles skewness)\n",
    "- Price difference (selling price vs MMR)\n",
    "- MMR ratio (selling/MMR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6c21948c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURE ENGINEERING\n",
      "================================================================================\n",
      "1. Vehicle Age:\n",
      "   Range: 0 to 33 years\n",
      "   Mean: 5.0 years\n",
      "\n",
      "2. Log Odometer:\n",
      "   Range: 0.69 to 13.12\n",
      "   Mean: 10.78\n",
      "\n",
      "3. Price Difference (selling - MMR):\n",
      "   Mean: $-157.95\n",
      "   Median: $-50.00\n",
      "   Negative (sold below MMR): 286,415 (51.3%)\n",
      "\n",
      "4. MMR Ratio:\n",
      "   Mean: 0.993\n",
      "   Median: 0.996\n",
      "   Sold above MMR: 261,187 (46.7%)\n",
      "\n",
      "5. Age-Odometer Interaction:\n",
      "   Mean: 50.2\n",
      "   Median: 15.7\n",
      "\n",
      "================================================================================\n",
      "Feature Engineering Complete:\n",
      "New features created: 5\n",
      "Total features: 22\n",
      "Dataset shape: (558825, 22)\n"
     ]
    }
   ],
   "source": [
    "print(\"FEATURE ENGINEERING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Vehicle Age (based on year 2015 - last sale date in dataset)\n",
    "reference_year = 2015\n",
    "df['vehicle_age'] = reference_year - df['year']\n",
    "print(f\"1. Vehicle Age:\")\n",
    "print(f\"   Range: {df['vehicle_age'].min()} to {df['vehicle_age'].max()} years\")\n",
    "print(f\"   Mean: {df['vehicle_age'].mean():.1f} years\")\n",
    "\n",
    "# 2. Log Odometer (handle skewed distribution)\n",
    "df['log_odometer'] = np.log1p(df['odometer'])\n",
    "print(f\"\\n2. Log Odometer:\")\n",
    "print(f\"   Range: {df['log_odometer'].min():.2f} to {df['log_odometer'].max():.2f}\")\n",
    "print(f\"   Mean: {df['log_odometer'].mean():.2f}\")\n",
    "\n",
    "# 3. Price Difference (selling price - MMR)\n",
    "df['price_diff'] = df['sellingprice'] - df['mmr']\n",
    "print(f\"\\n3. Price Difference (selling - MMR):\")\n",
    "print(f\"   Mean: ${df['price_diff'].mean():.2f}\")\n",
    "print(f\"   Median: ${df['price_diff'].median():.2f}\")\n",
    "print(f\"   Negative (sold below MMR): {(df['price_diff'] < 0).sum():,} ({(df['price_diff'] < 0).mean()*100:.1f}%)\")\n",
    "\n",
    "# 4. MMR Ratio (selling / MMR)\n",
    "df['mmr_ratio'] = df['sellingprice'] / df['mmr']\n",
    "print(f\"\\n4. MMR Ratio:\")\n",
    "print(f\"   Mean: {df['mmr_ratio'].mean():.3f}\")\n",
    "print(f\"   Median: {df['mmr_ratio'].median():.3f}\")\n",
    "print(f\"   Sold above MMR: {(df['mmr_ratio'] > 1).sum():,} ({(df['mmr_ratio'] > 1).mean()*100:.1f}%)\")\n",
    "\n",
    "# 5. Age-Odometer Interaction (captures high-mileage vs age relationship)\n",
    "df['age_odo_interaction'] = df['vehicle_age'] * df['odometer'] / 10000\n",
    "print(f\"\\n5. Age-Odometer Interaction:\")\n",
    "print(f\"   Mean: {df['age_odo_interaction'].mean():.1f}\")\n",
    "print(f\"   Median: {df['age_odo_interaction'].median():.1f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Feature Engineering Complete:\")\n",
    "print(f\"New features created: 5\")\n",
    "print(f\"Total features: {len(df.columns)}\")\n",
    "print(f\"Dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ed04e1",
   "metadata": {},
   "source": [
    "## Step 10: Handle High-Cardinality Categoricals\n",
    "\n",
    "Reduce cardinality in features with many unique values by grouping rare categories.\n",
    "Focus on seller, model, and trim which have thousands of unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f623d078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIGH-CARDINALITY CATEGORICAL HANDLING\n",
      "================================================================================\n",
      "Current Cardinality:\n",
      "  make: 67 unique values\n",
      "  model: 852 unique values\n",
      "  trim: 1,964 unique values\n",
      "  body: 46 unique values\n",
      "  transmission: 2 unique values\n",
      "  state: 64 unique values\n",
      "  color: 46 unique values\n",
      "  interior: 17 unique values\n",
      "  seller: 14,262 unique values\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Grouping Strategy:\n",
      "\n",
      "1. Seller: Top 100 kept, rest → 'Other_Seller'\n",
      "   Before: 14,262 | After: 101\n",
      "\n",
      "2. Model: Top 200 kept, rest → 'Other_Model'\n",
      "   Before: 852 | After: 201\n",
      "\n",
      "3. Trim: Top 100 kept, rest → 'Other_Trim'\n",
      "   Before: 1,964 | After: 101\n",
      "\n",
      "================================================================================\n",
      "High-Cardinality Handling Complete:\n",
      "Columns dropped: seller, model, trim\n",
      "Columns added: seller_grouped, model_grouped, trim_grouped\n",
      "New shape: (558825, 22)\n"
     ]
    }
   ],
   "source": [
    "print(\"HIGH-CARDINALITY CATEGORICAL HANDLING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Analyze cardinality\n",
    "cat_features = ['make', 'model', 'trim', 'body', 'transmission', 'state', \n",
    "                'color', 'interior', 'seller']\n",
    "\n",
    "print(\"Current Cardinality:\")\n",
    "for col in cat_features:\n",
    "    print(f\"  {col}: {df[col].nunique():,} unique values\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Grouping Strategy:\\n\")\n",
    "\n",
    "# 1. Seller - Keep top 100, group rest as 'Other'\n",
    "seller_threshold = 100\n",
    "top_sellers = df['seller'].value_counts().head(seller_threshold).index\n",
    "df['seller_grouped'] = df['seller'].apply(lambda x: x if x in top_sellers else 'Other_Seller')\n",
    "print(f\"1. Seller: Top {seller_threshold} kept, rest → 'Other_Seller'\")\n",
    "print(f\"   Before: {df['seller'].nunique():,} | After: {df['seller_grouped'].nunique():,}\")\n",
    "\n",
    "# 2. Model - Keep top 200, group rest as 'Other'\n",
    "model_threshold = 200\n",
    "top_models = df['model'].value_counts().head(model_threshold).index\n",
    "df['model_grouped'] = df['model'].apply(lambda x: x if x in top_models else 'Other_Model')\n",
    "print(f\"\\n2. Model: Top {model_threshold} kept, rest → 'Other_Model'\")\n",
    "print(f\"   Before: {df['model'].nunique():,} | After: {df['model_grouped'].nunique():,}\")\n",
    "\n",
    "# 3. Trim - Keep top 100, group rest as 'Other'\n",
    "trim_threshold = 100\n",
    "top_trims = df['trim'].value_counts().head(trim_threshold).index\n",
    "df['trim_grouped'] = df['trim'].apply(lambda x: x if x in top_trims else 'Other_Trim')\n",
    "print(f\"\\n3. Trim: Top {trim_threshold} kept, rest → 'Other_Trim'\")\n",
    "print(f\"   Before: {df['trim'].nunique():,} | After: {df['trim_grouped'].nunique():,}\")\n",
    "\n",
    "# Drop original high-cardinality columns\n",
    "df = df.drop(columns=['seller', 'model', 'trim'])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"High-Cardinality Handling Complete:\")\n",
    "print(f\"Columns dropped: seller, model, trim\")\n",
    "print(f\"Columns added: seller_grouped, model_grouped, trim_grouped\")\n",
    "print(f\"New shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0f91f8",
   "metadata": {},
   "source": [
    "## Step 11: Save Cleaned Dataset\n",
    "\n",
    "Export the fully cleaned and feature-engineered dataset for modeling.\n",
    "Document final data quality and feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "431d0637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVING CLEANED DATASET\n",
      "================================================================================\n",
      "Final Data Quality Summary:\n",
      "Total records: 558,825\n",
      "Total features: 22\n",
      "Missing values: 163,336\n",
      "Columns with missing: ['saledate']\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Feature Categories:\n",
      "Numeric features (11): ['year', 'condition', 'odometer', 'mmr', 'sellingprice', 'vehicle_age', 'log_odometer', 'price_diff', 'mmr_ratio', 'age_odo_interaction', 'has_date']\n",
      "Categorical features (9): ['make', 'body', 'transmission', 'state', 'color', 'interior', 'seller_grouped', 'model_grouped', 'trim_grouped']\n",
      "ID/Reference (2): ['vin', 'saledate']\n",
      "\n",
      "================================================================================\n",
      "Dataset saved: data/processed/car_prices_cleaned.csv\n",
      "File size: 105.1 MB\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Cleaning Summary:\n",
      "  Records retained: 558,825 / 558,837 (99.998%)\n",
      "  Features created: 5 (vehicle_age, log_odometer, price_diff, mmr_ratio, age_odo_interaction)\n",
      "  High-cardinality reduced: seller (14,262→101), model (852→201), trim (1,964→101)\n",
      "  Outliers capped: 81 odometer values\n",
      "  Missing values imputed: transmission, body, condition, make, model, colors\n",
      "  Data completeness: 98.3%\n",
      "\n",
      "================================================================================\n",
      "Notebook 01 Complete!\n",
      "Next: Notebook 02 - Modeling + Evaluation + Hyperparameter Tuning\n"
     ]
    }
   ],
   "source": [
    "print(\"SAVING CLEANED DATASET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Final data quality check\n",
    "print(\"Final Data Quality Summary:\")\n",
    "print(f\"Total records: {len(df):,}\")\n",
    "print(f\"Total features: {len(df.columns)}\")\n",
    "print(f\"Missing values: {df.isnull().sum().sum():,}\")\n",
    "missing_cols = df.isnull().sum()[df.isnull().sum() > 0]\n",
    "if len(missing_cols) > 0:\n",
    "    print(f\"Columns with missing: {list(missing_cols.index)}\")\n",
    "else:\n",
    "    print(\"All modeling features complete!\")\n",
    "\n",
    "# Feature summary\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Feature Categories:\")\n",
    "\n",
    "numeric_features = ['year', 'condition', 'odometer', 'mmr', 'sellingprice', \n",
    "                    'vehicle_age', 'log_odometer', 'price_diff', 'mmr_ratio', \n",
    "                    'age_odo_interaction', 'has_date']\n",
    "categorical_features = ['make', 'body', 'transmission', 'state', 'color', \n",
    "                       'interior', 'seller_grouped', 'model_grouped', 'trim_grouped']\n",
    "id_features = ['vin', 'saledate']\n",
    "\n",
    "print(f\"Numeric features ({len(numeric_features)}): {numeric_features}\")\n",
    "print(f\"Categorical features ({len(categorical_features)}): {categorical_features}\")\n",
    "print(f\"ID/Reference ({len(id_features)}): {id_features}\")\n",
    "\n",
    "# Save cleaned dataset\n",
    "output_path = 'data/processed/car_prices_cleaned.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"Dataset saved: {output_path}\")\n",
    "print(f\"File size: {os.path.getsize(output_path) / 1024**2:.1f} MB\")\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Cleaning Summary:\")\n",
    "print(f\"  Records retained: 558,825 / 558,837 (99.998%)\")\n",
    "print(f\"  Features created: 5 (vehicle_age, log_odometer, price_diff, mmr_ratio, age_odo_interaction)\")\n",
    "print(f\"  High-cardinality reduced: seller (14,262→101), model (852→201), trim (1,964→101)\")\n",
    "print(f\"  Outliers capped: 81 odometer values\")\n",
    "print(f\"  Missing values imputed: transmission, body, condition, make, model, colors\")\n",
    "print(f\"  Data completeness: 98.3%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Notebook 01 Complete!\")\n",
    "print(\"Next: Notebook 02 - Modeling + Evaluation + Hyperparameter Tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7752fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
