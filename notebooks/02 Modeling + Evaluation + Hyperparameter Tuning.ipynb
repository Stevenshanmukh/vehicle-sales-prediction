{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88bb3708",
   "metadata": {},
   "source": [
    "# Notebook 02: Modeling + Evaluation + Hyperparameter Tuning\n",
    "\n",
    "**Project:** Vehicle Sales & Market Insights  \n",
    "**Dataset:** car_prices_cleaned.csv (from Notebook 01)  \n",
    "**Purpose:** Build, evaluate, and optimize regression models for price prediction\n",
    "\n",
    "## Objective\n",
    "Develop production-ready price prediction models:\n",
    "- Train/validation/test split (time-aware)\n",
    "- Baseline models (Linear, Ridge, Random Forest)\n",
    "- Advanced models (LightGBM, CatBoost, XGBoost)\n",
    "- Hyperparameter optimization with Optuna\n",
    "- Model evaluation and selection\n",
    "- Save final model for deployment\n",
    "\n",
    "## Step 1: Environment Setup\n",
    "Load libraries and cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21f0bb17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Setup Complete\n",
      "Python libraries loaded successfully\n",
      "\n",
      "Dataset loaded: (558825, 22)\n",
      "Memory: 429.7 MB\n",
      "Target variable: sellingprice\n",
      "Features: 21\n"
     ]
    }
   ],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Modeling\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "# Advanced models\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "# Hyperparameter tuning\n",
    "import optuna\n",
    "\n",
    "# Utilities\n",
    "import pickle\n",
    "import warnings\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings('ignore')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "print(\"Environment Setup Complete\")\n",
    "print(f\"Python libraries loaded successfully\")\n",
    "\n",
    "# Load cleaned dataset\n",
    "df = pd.read_csv('data/processed/car_prices_cleaned.csv')\n",
    "\n",
    "print(f\"\\nDataset loaded: {df.shape}\")\n",
    "print(f\"Memory: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "print(f\"Target variable: sellingprice\")\n",
    "print(f\"Features: {len(df.columns) - 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcc41de",
   "metadata": {},
   "source": [
    "## Step 2: Feature Type Identification & Data Preparation\n",
    "\n",
    "Identify feature types and prepare data for modeling.\n",
    "Separate numeric features, categorical features, and non-modeling columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ee5b48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURE TYPE IDENTIFICATION\n",
      "================================================================================\n",
      "Total columns: 22\n",
      "Column names: ['year', 'make', 'body', 'transmission', 'vin', 'state', 'condition', 'odometer', 'color', 'interior', 'mmr', 'sellingprice', 'saledate', 'has_date', 'vehicle_age', 'log_odometer', 'price_diff', 'mmr_ratio', 'age_odo_interaction', 'seller_grouped', 'model_grouped', 'trim_grouped']\n",
      "\n",
      "Feature Categories:\n",
      "\n",
      "Numeric features (10):\n",
      "  - year\n",
      "  - condition\n",
      "  - odometer\n",
      "  - mmr\n",
      "  - vehicle_age\n",
      "  - log_odometer\n",
      "  - price_diff\n",
      "  - mmr_ratio\n",
      "  - age_odo_interaction\n",
      "  - has_date\n",
      "\n",
      "Categorical features (9):\n",
      "  - make (67 unique)\n",
      "  - body (46 unique)\n",
      "  - transmission (2 unique)\n",
      "  - state (64 unique)\n",
      "  - color (46 unique)\n",
      "  - interior (17 unique)\n",
      "  - seller_grouped (101 unique)\n",
      "  - model_grouped (201 unique)\n",
      "  - trim_grouped (101 unique)\n",
      "\n",
      "ID/Reference features (2):\n",
      "  - vin\n",
      "  - saledate\n",
      "\n",
      "Target: sellingprice\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Missing Values Check:\n",
      "All modeling features are complete!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Target Variable Statistics:\n",
      "Mean: $13,611.36\n",
      "Median: $12,100.00\n",
      "Std: $9,749.50\n",
      "Min: $1.00\n",
      "Max: $230,000.00\n",
      "Missing: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"FEATURE TYPE IDENTIFICATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Display all columns\n",
    "print(f\"Total columns: {len(df.columns)}\")\n",
    "print(f\"Column names: {list(df.columns)}\\n\")\n",
    "\n",
    "# Define feature types\n",
    "target = 'sellingprice'\n",
    "\n",
    "numeric_features = ['year', 'condition', 'odometer', 'mmr', 'vehicle_age', \n",
    "                    'log_odometer', 'price_diff', 'mmr_ratio', \n",
    "                    'age_odo_interaction', 'has_date']\n",
    "\n",
    "categorical_features = ['make', 'body', 'transmission', 'state', 'color', \n",
    "                       'interior', 'seller_grouped', 'model_grouped', \n",
    "                       'trim_grouped']\n",
    "\n",
    "id_features = ['vin', 'saledate']\n",
    "\n",
    "# Verify all columns accounted for\n",
    "all_defined = set(numeric_features + categorical_features + id_features + [target])\n",
    "all_actual = set(df.columns)\n",
    "missing = all_actual - all_defined\n",
    "extra = all_defined - all_actual\n",
    "\n",
    "print(\"Feature Categories:\")\n",
    "print(f\"\\nNumeric features ({len(numeric_features)}):\")\n",
    "for feat in numeric_features:\n",
    "    print(f\"  - {feat}\")\n",
    "\n",
    "print(f\"\\nCategorical features ({len(categorical_features)}):\")\n",
    "for feat in categorical_features:\n",
    "    print(f\"  - {feat} ({df[feat].nunique()} unique)\")\n",
    "\n",
    "print(f\"\\nID/Reference features ({len(id_features)}):\")\n",
    "for feat in id_features:\n",
    "    print(f\"  - {feat}\")\n",
    "\n",
    "print(f\"\\nTarget: {target}\")\n",
    "\n",
    "if missing:\n",
    "    print(f\"\\nWarning: Columns not categorized: {missing}\")\n",
    "if extra:\n",
    "    print(f\"\\nWarning: Defined but not in data: {extra}\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Missing Values Check:\")\n",
    "missing_counts = df[numeric_features + categorical_features].isnull().sum()\n",
    "if missing_counts.sum() == 0:\n",
    "    print(\"All modeling features are complete!\")\n",
    "else:\n",
    "    print(missing_counts[missing_counts > 0])\n",
    "\n",
    "# Target variable statistics\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Target Variable Statistics:\")\n",
    "print(f\"Mean: ${df[target].mean():,.2f}\")\n",
    "print(f\"Median: ${df[target].median():,.2f}\")\n",
    "print(f\"Std: ${df[target].std():,.2f}\")\n",
    "print(f\"Min: ${df[target].min():,.2f}\")\n",
    "print(f\"Max: ${df[target].max():,.2f}\")\n",
    "print(f\"Missing: {df[target].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65264028",
   "metadata": {},
   "source": [
    "## Step 3: Encode Categorical Features\n",
    "\n",
    "Apply label encoding to categorical features for tree-based models.\n",
    "These models handle encoded categories naturally without one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdfafa88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CATEGORICAL FEATURE ENCODING\n",
      "================================================================================\n",
      "Encoding categorical features:\n",
      "\n",
      "make                :  67 categories → encoded as 0-66\n",
      "body                :  46 categories → encoded as 0-45\n",
      "transmission        :   2 categories → encoded as 0-1\n",
      "state               :  64 categories → encoded as 0-63\n",
      "color               :  46 categories → encoded as 0-45\n",
      "interior            :  17 categories → encoded as 0-16\n",
      "seller_grouped      : 101 categories → encoded as 0-100\n",
      "model_grouped       : 201 categories → encoded as 0-200\n",
      "trim_grouped        : 101 categories → encoded as 0-100\n",
      "\n",
      "Label encoders saved: models/preprocessing/label_encoders.pkl\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Feature Matrix Prepared:\n",
      "X shape: (558825, 19)\n",
      "y shape: (558825,)\n",
      "\n",
      "Feature columns (19):\n",
      "  Numeric: ['year', 'condition', 'odometer', 'mmr', 'vehicle_age', 'log_odometer', 'price_diff', 'mmr_ratio', 'age_odo_interaction', 'has_date']\n",
      "  Categorical (encoded): ['make', 'body', 'transmission', 'state', 'color', 'interior', 'seller_grouped', 'model_grouped', 'trim_grouped']\n",
      "\n",
      "Missing values in X: 0\n",
      "Missing values in y: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"CATEGORICAL FEATURE ENCODING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create copy for encoding\n",
    "df_encoded = df.copy()\n",
    "\n",
    "# Label encode categorical features\n",
    "print(\"Encoding categorical features:\\n\")\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    df_encoded[col] = le.fit_transform(df_encoded[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "    print(f\"{col:20s}: {df[col].nunique():3d} categories → encoded as 0-{df_encoded[col].max()}\")\n",
    "\n",
    "# Save encoders for later use\n",
    "encoders_path = 'models/preprocessing/label_encoders.pkl'\n",
    "os.makedirs('models/preprocessing', exist_ok=True)\n",
    "with open(encoders_path, 'wb') as f:\n",
    "    pickle.dump(label_encoders, f)\n",
    "\n",
    "print(f\"\\nLabel encoders saved: {encoders_path}\")\n",
    "\n",
    "# Prepare feature matrix and target\n",
    "X = df_encoded[numeric_features + categorical_features]\n",
    "y = df_encoded[target]\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Feature Matrix Prepared:\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"\\nFeature columns ({len(X.columns)}):\")\n",
    "print(f\"  Numeric: {numeric_features}\")\n",
    "print(f\"  Categorical (encoded): {categorical_features}\")\n",
    "\n",
    "# Verify no missing values\n",
    "print(f\"\\nMissing values in X: {X.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in y: {y.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38bd88b",
   "metadata": {},
   "source": [
    "## Step 4: Train/Validation/Test Split\n",
    "\n",
    "Create train/validation/test splits with stratified sampling.\n",
    "Use 70% train, 15% validation, 15% test to ensure sufficient data for all sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21c8d0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN/VALIDATION/TEST SPLIT\n",
      "================================================================================\n",
      "Split Sizes:\n",
      "  Training set:   391,177 (70.0%)\n",
      "  Validation set: 83,824 (15.0%)\n",
      "  Test set:       83,824 (15.0%)\n",
      "  Total:          558,825\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Target Distribution Across Splits:\n",
      "  Train   - Mean: $13,605.71, Std: $9,756.17\n",
      "  Val     - Mean: $13,613.10, Std: $9,681.98\n",
      "  Test    - Mean: $13,635.96, Std: $9,785.69\n",
      "  Overall - Mean: $13,611.36, Std: $9,749.50\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Feature Statistics (Training Set):\n",
      "  Numeric features mean odometer: 68,311\n",
      "  Numeric features mean vehicle_age: 5.0\n",
      "  Numeric features mean mmr: $13,764.58\n",
      "\n",
      "================================================================================\n",
      "Data split complete and ready for modeling!\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAIN/VALIDATION/TEST SPLIT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Split: 70% train, 15% validation, 15% test\n",
    "# First split: 70% train, 30% temp\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Second split: 50% of temp for validation, 50% for test (15% each of total)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"Split Sizes:\")\n",
    "print(f\"  Training set:   {len(X_train):,} ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"  Validation set: {len(X_val):,} ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "print(f\"  Test set:       {len(X_test):,} ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "print(f\"  Total:          {len(X):,}\")\n",
    "\n",
    "# Verify target distribution across splits\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Target Distribution Across Splits:\")\n",
    "print(f\"  Train   - Mean: ${y_train.mean():,.2f}, Std: ${y_train.std():,.2f}\")\n",
    "print(f\"  Val     - Mean: ${y_val.mean():,.2f}, Std: ${y_val.std():,.2f}\")\n",
    "print(f\"  Test    - Mean: ${y_test.mean():,.2f}, Std: ${y_test.std():,.2f}\")\n",
    "print(f\"  Overall - Mean: ${y.mean():,.2f}, Std: ${y.std():,.2f}\")\n",
    "\n",
    "# Check feature distributions\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Feature Statistics (Training Set):\")\n",
    "print(f\"  Numeric features mean odometer: {X_train['odometer'].mean():,.0f}\")\n",
    "print(f\"  Numeric features mean vehicle_age: {X_train['vehicle_age'].mean():.1f}\")\n",
    "print(f\"  Numeric features mean mmr: ${X_train['mmr'].mean():,.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Data split complete and ready for modeling!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3461e856",
   "metadata": {},
   "source": [
    "## Step 5: Train Baseline Models\n",
    "\n",
    "Establish performance baselines with simple models:\n",
    "- Linear Regression (baseline)\n",
    "- Ridge Regression (regularized linear)\n",
    "- Random Forest (tree-based ensemble)\n",
    "\n",
    "Evaluate using MAE, RMSE, R², and MAPE metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b06ef928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASELINE MODEL TRAINING\n",
      "================================================================================\n",
      "1. Training Linear Regression...\n",
      "   ✓ Complete\n",
      "2. Training Ridge Regression...\n",
      "   ✓ Complete\n",
      "3. Training Random Forest...\n",
      "   ✓ Complete\n",
      "\n",
      "================================================================================\n",
      "BASELINE MODEL RESULTS (Validation Set)\n",
      "================================================================================\n",
      "            Model          MAE         RMSE       R2         MAPE\n",
      "Linear Regression 1.458705e-11 1.983436e-11 1.000000 2.559347e-13\n",
      "            Ridge 1.006473e-09 1.631534e-09 1.000000 1.458081e-11\n",
      "    Random Forest 1.386135e+01 1.631839e+02 0.999716 1.391432e-01\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Interpretation:\n",
      "  Best MAE:  Linear Regression ($0.00)\n",
      "  Best RMSE: Linear Regression ($0.00)\n",
      "  Best R²:   Linear Regression (1.0000)\n",
      "  Best MAPE: Linear Regression (0.00%)\n"
     ]
    }
   ],
   "source": [
    "print(\"BASELINE MODEL TRAINING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define evaluation function\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n",
    "    \n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'R2': r2,\n",
    "        'MAPE': mape\n",
    "    }\n",
    "\n",
    "results = []\n",
    "\n",
    "# 1. Linear Regression\n",
    "print(\"1. Training Linear Regression...\")\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_val)\n",
    "results.append(evaluate_model(y_val, y_pred_lr, 'Linear Regression'))\n",
    "print(\"   ✓ Complete\")\n",
    "\n",
    "# 2. Ridge Regression\n",
    "print(\"2. Training Ridge Regression...\")\n",
    "ridge = Ridge(alpha=1.0, random_state=RANDOM_STATE)\n",
    "ridge.fit(X_train, y_train)\n",
    "y_pred_ridge = ridge.predict(X_val)\n",
    "results.append(evaluate_model(y_val, y_pred_ridge, 'Ridge'))\n",
    "print(\"   ✓ Complete\")\n",
    "\n",
    "# 3. Random Forest\n",
    "print(\"3. Training Random Forest...\")\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    min_samples_split=20,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_val)\n",
    "results.append(evaluate_model(y_val, y_pred_rf, 'Random Forest'))\n",
    "print(\"   ✓ Complete\")\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BASELINE MODEL RESULTS (Validation Set)\")\n",
    "print(\"=\" * 80)\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Interpretation:\")\n",
    "print(f\"  Best MAE:  {results_df.loc[results_df['MAE'].idxmin(), 'Model']} (${results_df['MAE'].min():,.2f})\")\n",
    "print(f\"  Best RMSE: {results_df.loc[results_df['RMSE'].idxmin(), 'Model']} (${results_df['RMSE'].min():,.2f})\")\n",
    "print(f\"  Best R²:   {results_df.loc[results_df['R2'].idxmax(), 'Model']} ({results_df['R2'].max():.4f})\")\n",
    "print(f\"  Best MAPE: {results_df.loc[results_df['MAPE'].idxmin(), 'Model']} ({results_df['MAPE'].min():.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c9b615",
   "metadata": {},
   "source": [
    "## Step 6: Fix Data Leakage\n",
    "\n",
    "Remove features derived from the target variable (price_diff, mmr_ratio).\n",
    "These cause data leakage and artificial perfect predictions.\n",
    "Retrain baseline models with clean features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b6b1862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIXING DATA LEAKAGE\n",
      "================================================================================\n",
      "Identifying leakage features:\n",
      "  - price_diff = sellingprice - mmr (LEAKAGE)\n",
      "  - mmr_ratio = sellingprice / mmr (LEAKAGE)\n",
      "\n",
      "These features are derived FROM the target and must be removed.\n",
      "\n",
      "Numeric features before: 10\n",
      "Numeric features after:  8\n",
      "\n",
      "New feature matrix: (558825, 17)\n",
      "Features removed: ['price_diff', 'mmr_ratio']\n",
      "Remaining features (17): ['year', 'condition', 'odometer', 'mmr', 'vehicle_age', 'log_odometer', 'age_odo_interaction', 'has_date', 'make', 'body', 'transmission', 'state', 'color', 'interior', 'seller_grouped', 'model_grouped', 'trim_grouped']\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Data re-split with clean features:\n",
      "  Training:   (391177, 17)\n",
      "  Validation: (83824, 17)\n",
      "  Test:       (83824, 17)\n"
     ]
    }
   ],
   "source": [
    "print(\"FIXING DATA LEAKAGE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"Identifying leakage features:\")\n",
    "print(\"  - price_diff = sellingprice - mmr (LEAKAGE)\")\n",
    "print(\"  - mmr_ratio = sellingprice / mmr (LEAKAGE)\")\n",
    "print(\"\\nThese features are derived FROM the target and must be removed.\")\n",
    "\n",
    "# Remove leakage features\n",
    "leakage_features = ['price_diff', 'mmr_ratio']\n",
    "numeric_features_clean = [f for f in numeric_features if f not in leakage_features]\n",
    "\n",
    "print(f\"\\nNumeric features before: {len(numeric_features)}\")\n",
    "print(f\"Numeric features after:  {len(numeric_features_clean)}\")\n",
    "\n",
    "# Recreate feature matrix without leakage\n",
    "X_clean = df_encoded[numeric_features_clean + categorical_features]\n",
    "\n",
    "print(f\"\\nNew feature matrix: {X_clean.shape}\")\n",
    "print(f\"Features removed: {leakage_features}\")\n",
    "print(f\"Remaining features ({len(X_clean.columns)}): {list(X_clean.columns)}\")\n",
    "\n",
    "# Re-split data\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_clean, y, test_size=0.30, random_state=RANDOM_STATE\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Data re-split with clean features:\")\n",
    "print(f\"  Training:   {X_train.shape}\")\n",
    "print(f\"  Validation: {X_val.shape}\")\n",
    "print(f\"  Test:       {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f02eea",
   "metadata": {},
   "source": [
    "## Step 7: Retrain Baseline Models with Clean Features\n",
    "\n",
    "Train baseline models again without data leakage.\n",
    "This will show realistic model performance for price prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a741dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASELINE MODEL TRAINING (CLEAN FEATURES)\n",
      "================================================================================\n",
      "1. Training Linear Regression...\n",
      "   ✓ Complete\n",
      "2. Training Ridge Regression...\n",
      "   ✓ Complete\n",
      "3. Training Random Forest...\n",
      "   ✓ Complete\n",
      "\n",
      "================================================================================\n",
      "BASELINE MODEL RESULTS (Validation Set - Clean Features)\n",
      "================================================================================\n",
      "            Model         MAE        RMSE       R2      MAPE\n",
      "Linear Regression 1053.310882 1643.373110 0.971190 15.701325\n",
      "            Ridge 1053.310476 1643.372976 0.971190 15.701273\n",
      "    Random Forest  932.418677 1477.742741 0.976704 13.446442\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Best Performers:\n",
      "  Best MAE:  Random Forest ($932.42)\n",
      "  Best RMSE: Random Forest ($1,477.74)\n",
      "  Best R²:   Random Forest (0.9767)\n",
      "  Best MAPE: Random Forest (13.45%)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Insights:\n",
      "  Random Forest shows strong performance with R² = 0.9767\n",
      "  Linear models have lower R² due to non-linear relationships in data\n",
      "  Tree-based models better capture complex feature interactions\n"
     ]
    }
   ],
   "source": [
    "print(\"BASELINE MODEL TRAINING (CLEAN FEATURES)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results_clean = []\n",
    "\n",
    "# 1. Linear Regression\n",
    "print(\"1. Training Linear Regression...\")\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_val)\n",
    "results_clean.append(evaluate_model(y_val, y_pred_lr, 'Linear Regression'))\n",
    "print(\"   ✓ Complete\")\n",
    "\n",
    "# 2. Ridge Regression\n",
    "print(\"2. Training Ridge Regression...\")\n",
    "ridge = Ridge(alpha=10.0, random_state=RANDOM_STATE)\n",
    "ridge.fit(X_train, y_train)\n",
    "y_pred_ridge = ridge.predict(X_val)\n",
    "results_clean.append(evaluate_model(y_val, y_pred_ridge, 'Ridge'))\n",
    "print(\"   ✓ Complete\")\n",
    "\n",
    "# 3. Random Forest\n",
    "print(\"3. Training Random Forest...\")\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    min_samples_split=20,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_val)\n",
    "results_clean.append(evaluate_model(y_val, y_pred_rf, 'Random Forest'))\n",
    "print(\"   ✓ Complete\")\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BASELINE MODEL RESULTS (Validation Set - Clean Features)\")\n",
    "print(\"=\" * 80)\n",
    "results_df = pd.DataFrame(results_clean)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Best Performers:\")\n",
    "print(f\"  Best MAE:  {results_df.loc[results_df['MAE'].idxmin(), 'Model']} (${results_df['MAE'].min():,.2f})\")\n",
    "print(f\"  Best RMSE: {results_df.loc[results_df['RMSE'].idxmin(), 'Model']} (${results_df['RMSE'].min():,.2f})\")\n",
    "print(f\"  Best R²:   {results_df.loc[results_df['R2'].idxmax(), 'Model']} ({results_df['R2'].max():.4f})\")\n",
    "print(f\"  Best MAPE: {results_df.loc[results_df['MAPE'].idxmin(), 'Model']} ({results_df['MAPE'].min():.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Insights:\")\n",
    "print(f\"  Random Forest shows strong performance with R² = {results_df.loc[results_df['Model']=='Random Forest', 'R2'].values[0]:.4f}\")\n",
    "print(f\"  Linear models have lower R² due to non-linear relationships in data\")\n",
    "print(f\"  Tree-based models better capture complex feature interactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1823b2",
   "metadata": {},
   "source": [
    "## Step 8: Train Advanced Gradient Boosting Models\n",
    "\n",
    "Train state-of-the-art gradient boosting models:\n",
    "- LightGBM (fast, efficient)\n",
    "- XGBoost (robust, widely used)\n",
    "- CatBoost (handles categoricals natively)\n",
    "\n",
    "Compare against Random Forest baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10517549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADVANCED MODEL TRAINING\n",
      "================================================================================\n",
      "1. Training LightGBM...\n",
      "   ✓ Complete\n",
      "2. Training XGBoost...\n",
      "   ✓ Complete\n",
      "3. Training CatBoost...\n",
      "   ✓ Complete\n",
      "\n",
      "================================================================================\n",
      "ALL MODEL RESULTS (Validation Set)\n",
      "================================================================================\n",
      "            Model         MAE        RMSE       R2      MAPE\n",
      "          XGBoost  905.979434 1567.303833 0.973795 12.733463\n",
      "         LightGBM  906.705690 1510.702516 0.975654 13.098833\n",
      "    Random Forest  932.418677 1477.742741 0.976704 13.446442\n",
      "         CatBoost  952.641952 1643.748533 0.971176 13.842275\n",
      "            Ridge 1053.310476 1643.372976 0.971190 15.701273\n",
      "Linear Regression 1053.310882 1643.373110 0.971190 15.701325\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Best Model by Metric:\n",
      "  Lowest MAE:  XGBoost ($905.98)\n",
      "  Lowest RMSE: Random Forest ($1,477.74)\n",
      "  Highest R²:  Random Forest (0.9767)\n",
      "  Lowest MAPE: XGBoost (12.73%)\n",
      "\n",
      "================================================================================\n",
      "Champion Model: XGBoost\n",
      "Performance: $905.98 MAE, 0.9738 R²\n"
     ]
    }
   ],
   "source": [
    "print(\"ADVANCED MODEL TRAINING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. LightGBM\n",
    "print(\"1. Training LightGBM...\")\n",
    "lgb_model = lgb.LGBMRegressor(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=15,\n",
    "    num_leaves=100,\n",
    "    min_child_samples=20,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "lgb_model.fit(X_train, y_train)\n",
    "y_pred_lgb = lgb_model.predict(X_val)\n",
    "results_clean.append(evaluate_model(y_val, y_pred_lgb, 'LightGBM'))\n",
    "print(\"   ✓ Complete\")\n",
    "\n",
    "# 2. XGBoost\n",
    "print(\"2. Training XGBoost...\")\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=10,\n",
    "    min_child_weight=5,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    verbosity=0\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_model.predict(X_val)\n",
    "results_clean.append(evaluate_model(y_val, y_pred_xgb, 'XGBoost'))\n",
    "print(\"   ✓ Complete\")\n",
    "\n",
    "# 3. CatBoost\n",
    "print(\"3. Training CatBoost...\")\n",
    "cat_model = CatBoostRegressor(\n",
    "    iterations=200,\n",
    "    learning_rate=0.05,\n",
    "    depth=10,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbose=0\n",
    ")\n",
    "cat_model.fit(X_train, y_train)\n",
    "y_pred_cat = cat_model.predict(X_val)\n",
    "results_clean.append(evaluate_model(y_val, y_pred_cat, 'CatBoost'))\n",
    "print(\"   ✓ Complete\")\n",
    "\n",
    "# Display all results\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ALL MODEL RESULTS (Validation Set)\")\n",
    "print(\"=\" * 80)\n",
    "results_df = pd.DataFrame(results_clean)\n",
    "results_df = results_df.sort_values('MAE')\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Best Model by Metric:\")\n",
    "print(f\"  Lowest MAE:  {results_df.iloc[0]['Model']} (${results_df.iloc[0]['MAE']:,.2f})\")\n",
    "print(f\"  Lowest RMSE: {results_df.loc[results_df['RMSE'].idxmin(), 'Model']} (${results_df['RMSE'].min():,.2f})\")\n",
    "print(f\"  Highest R²:  {results_df.loc[results_df['R2'].idxmax(), 'Model']} ({results_df['R2'].max():.4f})\")\n",
    "print(f\"  Lowest MAPE: {results_df.loc[results_df['MAPE'].idxmin(), 'Model']} ({results_df['MAPE'].min():.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"Champion Model: {results_df.iloc[0]['Model']}\")\n",
    "print(f\"Performance: ${results_df.iloc[0]['MAE']:,.2f} MAE, {results_df.iloc[0]['R2']:.4f} R²\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38625f0",
   "metadata": {},
   "source": [
    "## Step 9: Hyperparameter Optimization with Optuna\n",
    "\n",
    "Use Optuna to find optimal hyperparameters for XGBoost.\n",
    "Optimize for MAE (Mean Absolute Error) to minimize prediction error.\n",
    "Run 50 trials to balance search time and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d14a3d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYPERPARAMETER OPTIMIZATION WITH OPTUNA\n",
      "================================================================================\n",
      "Starting Optuna optimization...\n",
      "Metric to optimize: MAE (Mean Absolute Error)\n",
      "Number of trials: 50\n",
      "This may take 5-10 minutes...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "febf72ad36b94b9e828737c6876cf22c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "OPTIMIZATION RESULTS\n",
      "================================================================================\n",
      "Best MAE: $886.53\n",
      "\n",
      "Best Hyperparameters:\n",
      "  n_estimators: 448\n",
      "  learning_rate: 0.0452419039553789\n",
      "  max_depth: 12\n",
      "  min_child_weight: 6\n",
      "  subsample: 0.9393774616218383\n",
      "  colsample_bytree: 0.7938614515522494\n",
      "  gamma: 1.5790987446846159\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training final optimized model...\n",
      "✓ Complete\n",
      "\n",
      "================================================================================\n",
      "OPTIMIZED MODEL PERFORMANCE\n",
      "================================================================================\n",
      "MAE:  $886.53\n",
      "RMSE: $1,527.48\n",
      "R²:   0.9751\n",
      "MAPE: 12.40%\n",
      "\n",
      "Improvement over baseline XGBoost: $19.45 (2.15%)\n"
     ]
    }
   ],
   "source": [
    "print(\"HYPERPARAMETER OPTIMIZATION WITH OPTUNA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define objective function for Optuna\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
    "        'max_depth': trial.suggest_int('max_depth', 6, 15),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'n_jobs': -1,\n",
    "        'verbosity': 0\n",
    "    }\n",
    "    \n",
    "    model = xgb.XGBRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    \n",
    "    return mae\n",
    "\n",
    "print(\"Starting Optuna optimization...\")\n",
    "print(f\"Metric to optimize: MAE (Mean Absolute Error)\")\n",
    "print(f\"Number of trials: 50\")\n",
    "print(f\"This may take 5-10 minutes...\\n\")\n",
    "\n",
    "# Create study and optimize\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"OPTIMIZATION RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Best MAE: ${study.best_value:,.2f}\")\n",
    "print(f\"\\nBest Hyperparameters:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Train final model with best parameters\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Training final optimized model...\")\n",
    "best_params = study.best_params\n",
    "best_params.update({'random_state': RANDOM_STATE, 'n_jobs': -1, 'verbosity': 0})\n",
    "\n",
    "xgb_optimized = xgb.XGBRegressor(**best_params)\n",
    "xgb_optimized.fit(X_train, y_train)\n",
    "y_pred_optimized = xgb_optimized.predict(X_val)\n",
    "\n",
    "print(\"✓ Complete\")\n",
    "\n",
    "# Evaluate optimized model\n",
    "result_optimized = evaluate_model(y_val, y_pred_optimized, 'XGBoost (Optimized)')\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"OPTIMIZED MODEL PERFORMANCE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"MAE:  ${result_optimized['MAE']:,.2f}\")\n",
    "print(f\"RMSE: ${result_optimized['RMSE']:,.2f}\")\n",
    "print(f\"R²:   {result_optimized['R2']:.4f}\")\n",
    "print(f\"MAPE: {result_optimized['MAPE']:.2f}%\")\n",
    "\n",
    "# Compare with baseline XGBoost\n",
    "baseline_mae = results_df[results_df['Model'] == 'XGBoost']['MAE'].values[0]\n",
    "improvement = baseline_mae - result_optimized['MAE']\n",
    "print(f\"\\nImprovement over baseline XGBoost: ${improvement:,.2f} ({improvement/baseline_mae*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5954ad92",
   "metadata": {},
   "source": [
    "## Step 10: Final Model Evaluation on Test Set\n",
    "\n",
    "Evaluate the optimized XGBoost model on the held-out test set.\n",
    "This confirms the model generalizes well to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b26ad02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL MODEL EVALUATION ON TEST SET\n",
      "================================================================================\n",
      "Test Set Performance:\n",
      "  MAE:  $887.48\n",
      "  RMSE: $1,745.86\n",
      "  R²:   0.9682\n",
      "  MAPE: 12.30%\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Validation vs Test Comparison:\n",
      "  MAE:  $886.53 (val) vs $887.48 (test)\n",
      "  RMSE: $1,527.48 (val) vs $1,745.86 (test)\n",
      "  R²:   0.9751 (val) vs 0.9682 (test)\n",
      "  MAPE: 12.40% (val) vs 12.30% (test)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Generalization Check:\n",
      "  ✓ Model generalizes well - minimal difference between validation and test\n",
      "  MAE difference: $0.96\n",
      "  R² difference: 0.0069\n",
      "\n",
      "================================================================================\n",
      "Sample Predictions (First 10 test samples):\n",
      "================================================================================\n",
      " Actual    Predicted       Error   Error_%\n",
      " 7000.0  6743.246094  256.753906  3.667913\n",
      " 8500.0  8949.000977 -449.000977 -5.282364\n",
      " 6900.0  5942.892578  957.107422 13.871122\n",
      " 1550.0   826.626648  723.373352 46.669249\n",
      "13700.0 13729.688477  -29.688477 -0.216704\n",
      "32200.0 30120.544922 2079.455078  6.457935\n",
      "10500.0 11053.075195 -553.075195 -5.267383\n",
      "15600.0 15879.174805 -279.174805 -1.789582\n",
      "27800.0 25917.992188 1882.007812  6.769812\n",
      "10500.0 10925.859375 -425.859375 -4.055804\n",
      "\n",
      "  Mean absolute error in sample: $763.55\n"
     ]
    }
   ],
   "source": [
    "print(\"FINAL MODEL EVALUATION ON TEST SET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_test = xgb_optimized.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "test_results = evaluate_model(y_test, y_pred_test, 'XGBoost (Optimized)')\n",
    "\n",
    "print(\"Test Set Performance:\")\n",
    "print(f\"  MAE:  ${test_results['MAE']:,.2f}\")\n",
    "print(f\"  RMSE: ${test_results['RMSE']:,.2f}\")\n",
    "print(f\"  R²:   {test_results['R2']:.4f}\")\n",
    "print(f\"  MAPE: {test_results['MAPE']:.2f}%\")\n",
    "\n",
    "# Compare validation vs test\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Validation vs Test Comparison:\")\n",
    "print(f\"  MAE:  ${result_optimized['MAE']:,.2f} (val) vs ${test_results['MAE']:,.2f} (test)\")\n",
    "print(f\"  RMSE: ${result_optimized['RMSE']:,.2f} (val) vs ${test_results['RMSE']:,.2f} (test)\")\n",
    "print(f\"  R²:   {result_optimized['R2']:.4f} (val) vs {test_results['R2']:.4f} (test)\")\n",
    "print(f\"  MAPE: {result_optimized['MAPE']:.2f}% (val) vs {test_results['MAPE']:.2f}% (test)\")\n",
    "\n",
    "# Check for overfitting\n",
    "mae_diff = abs(test_results['MAE'] - result_optimized['MAE'])\n",
    "r2_diff = abs(test_results['R2'] - result_optimized['R2'])\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Generalization Check:\")\n",
    "if mae_diff < 50 and r2_diff < 0.01:\n",
    "    print(\"  ✓ Model generalizes well - minimal difference between validation and test\")\n",
    "elif mae_diff < 100 and r2_diff < 0.02:\n",
    "    print(\"  ✓ Model generalizes reasonably - acceptable difference\")\n",
    "else:\n",
    "    print(\"  ⚠ Potential overfitting - significant performance drop on test set\")\n",
    "\n",
    "print(f\"  MAE difference: ${mae_diff:,.2f}\")\n",
    "print(f\"  R² difference: {r2_diff:.4f}\")\n",
    "\n",
    "# Prediction examples\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Sample Predictions (First 10 test samples):\")\n",
    "print(\"=\" * 80)\n",
    "sample_results = pd.DataFrame({\n",
    "    'Actual': y_test.values[:10],\n",
    "    'Predicted': y_pred_test[:10],\n",
    "    'Error': y_test.values[:10] - y_pred_test[:10],\n",
    "    'Error_%': ((y_test.values[:10] - y_pred_test[:10]) / y_test.values[:10] * 100)\n",
    "})\n",
    "print(sample_results.to_string(index=False))\n",
    "\n",
    "print(f\"\\n  Mean absolute error in sample: ${abs(sample_results['Error']).mean():,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246700c2",
   "metadata": {},
   "source": [
    "## Step 11: Save Final Model and Artifacts\n",
    "\n",
    "Save the trained model, label encoders, and metadata for deployment.\n",
    "Package everything needed for production inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f65071a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVING FINAL MODEL AND ARTIFACTS\n",
      "================================================================================\n",
      "✓ Model saved: models/final/xgboost_optimized.pkl\n",
      "✓ Metadata saved: models/final/model_metadata.pkl\n",
      "✓ Feature importance saved: models/final/feature_importance.csv\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "            feature  importance\n",
      "                mmr    0.616879\n",
      "               body    0.089662\n",
      "age_odo_interaction    0.084514\n",
      "               make    0.058557\n",
      "      model_grouped    0.030040\n",
      "               year    0.028413\n",
      "           odometer    0.027547\n",
      "          condition    0.012360\n",
      "       trim_grouped    0.012269\n",
      "       transmission    0.009032\n",
      "\n",
      "✓ Training config saved: models/final/training_config.pkl\n",
      "\n",
      "================================================================================\n",
      "MODEL DEPLOYMENT PACKAGE READY\n",
      "================================================================================\n",
      "Location: models/final/\n",
      "Files created:\n",
      "  1. xgboost_optimized.pkl (trained model)\n",
      "  2. model_metadata.pkl (performance metrics & config)\n",
      "  3. feature_importance.csv (feature rankings)\n",
      "  4. training_config.pkl (training parameters)\n",
      "  5. label_encoders.pkl (from Step 3, in models/preprocessing/)\n",
      "\n",
      "================================================================================\n",
      "NOTEBOOK 02 COMPLETE!\n",
      "================================================================================\n",
      "Final Model: XGBoost (Optimized)\n",
      "Test Performance: $887.48 MAE, 0.9682 R², 12.30% MAPE\n",
      "\n",
      "Next: Notebook 03 - Explainability + Fairness + Diagnostics\n"
     ]
    }
   ],
   "source": [
    "print(\"SAVING FINAL MODEL AND ARTIFACTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('models/final', exist_ok=True)\n",
    "\n",
    "# 1. Save the optimized model\n",
    "model_path = 'models/final/xgboost_optimized.pkl'\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(xgb_optimized, f)\n",
    "print(f\"✓ Model saved: {model_path}\")\n",
    "\n",
    "# 2. Save model metadata\n",
    "metadata = {\n",
    "    'model_type': 'XGBoost Regressor',\n",
    "    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'features': list(X_train.columns),\n",
    "    'numeric_features': numeric_features_clean,\n",
    "    'categorical_features': categorical_features,\n",
    "    'n_train_samples': len(X_train),\n",
    "    'n_val_samples': len(X_val),\n",
    "    'n_test_samples': len(X_test),\n",
    "    'hyperparameters': study.best_params,\n",
    "    'performance': {\n",
    "        'validation_mae': result_optimized['MAE'],\n",
    "        'validation_rmse': result_optimized['RMSE'],\n",
    "        'validation_r2': result_optimized['R2'],\n",
    "        'validation_mape': result_optimized['MAPE'],\n",
    "        'test_mae': test_results['MAE'],\n",
    "        'test_rmse': test_results['RMSE'],\n",
    "        'test_r2': test_results['R2'],\n",
    "        'test_mape': test_results['MAPE']\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_path = 'models/final/model_metadata.pkl'\n",
    "with open(metadata_path, 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "print(f\"✓ Metadata saved: {metadata_path}\")\n",
    "\n",
    "# 3. Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': xgb_optimized.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "importance_path = 'models/final/feature_importance.csv'\n",
    "feature_importance.to_csv(importance_path, index=False)\n",
    "print(f\"✓ Feature importance saved: {importance_path}\")\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.head(10).to_string(index=False))\n",
    "\n",
    "# 4. Save complete training configuration\n",
    "config = {\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'train_split': 0.70,\n",
    "    'val_split': 0.15,\n",
    "    'test_split': 0.15,\n",
    "    'removed_features': ['price_diff', 'mmr_ratio'],\n",
    "    'target': 'sellingprice'\n",
    "}\n",
    "\n",
    "config_path = 'models/final/training_config.pkl'\n",
    "with open(config_path, 'wb') as f:\n",
    "    pickle.dump(config, f)\n",
    "print(f\"\\n✓ Training config saved: {config_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL DEPLOYMENT PACKAGE READY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Location: models/final/\")\n",
    "print(f\"Files created:\")\n",
    "print(f\"  1. xgboost_optimized.pkl (trained model)\")\n",
    "print(f\"  2. model_metadata.pkl (performance metrics & config)\")\n",
    "print(f\"  3. feature_importance.csv (feature rankings)\")\n",
    "print(f\"  4. training_config.pkl (training parameters)\")\n",
    "print(f\"  5. label_encoders.pkl (from Step 3, in models/preprocessing/)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"NOTEBOOK 02 COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Final Model: XGBoost (Optimized)\")\n",
    "print(f\"Test Performance: ${test_results['MAE']:,.2f} MAE, {test_results['R2']:.4f} R², {test_results['MAPE']:.2f}% MAPE\")\n",
    "print(f\"\\nNext: Notebook 03 - Explainability + Fairness + Diagnostics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6967a0e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
